{
  "$id": "adaptive_learning_comprehensive_integration.json",
  "version": "3.0.0", 
  "description": "Comprehensive Adaptive Learning System Integration for zeroTrace AI - Supports DSA, UPSC, JEE, and Developer Learning with AI-Powered Personalization",
  "target_project": "zeroTrace AI - Enhanced Adaptive Learning Assistant",
  "source_inspiration": "Advanced Adaptive Learning Research + Reinforcement Learning + User Modeling + Content Authoring",
  "integration_scope": [
    "AI-Powered User Modeling with TensorFlow",
    "Reinforcement Learning for Adaptive Pathways", 
    "Multi-Domain Learning (DSA, UPSC, JEE, Development)",
    "Python-Based API Integration with Custom Models",
    "Content Authoring and LMS Integration",
    "Intelligent Question Generation and Assessment",
    "Real-time Progress Tracking and Analytics",
    "Privacy-First Architecture with Local Processing"
  ],
  "learning_domains": {
    "dsa_programming": ["algorithms", "data_structures", "competitive_programming", "system_design", "coding_interviews"],
    "upsc_preparation": ["general_studies", "current_affairs", "essay_writing", "prelims", "mains", "interview_prep"], 
    "jee_preparation": ["physics", "chemistry", "mathematics", "mock_tests", "problem_solving"],
    "developer_skills": ["web_development", "mobile_development", "devops", "cloud_computing", "ai_ml", "blockchain"]
  },
  "ai_integration": {
    "model_support": "Custom API endpoints with base URL and API key configuration",
    "supported_models": ["OpenAI GPT", "Anthropic Claude", "Google Gemini", "Local Models", "Custom Fine-tuned Models"],
    "processing_location": "Client-side with API calls for complex reasoning",
    "privacy_mode": "Local processing option with offline models"
  },
  "technical_stack": {
    "backend": "Python 3.9+",
    "ai_frameworks": ["TensorFlow", "scikit-learn", "numpy", "pandas"],
    "rl_libraries": ["gym", "stable-baselines3", "ray[rllib]"],
    "api_integration": ["requests", "aiohttp", "fastapi"],
    "data_processing": ["pandas", "numpy", "matplotlib", "seaborn"],
    "storage": ["sqlite3", "json", "pickle", "chrome.storage.local"]
  },
  "actions": [
    {
      "type": "writeFile",
      "path": "src/adaptive_learning/config.py",
      "ifExists": "overwrite",
      "description": "Configuration management for adaptive learning system",
      "content": "# Adaptive Learning Configuration\nimport os\nfrom typing import Dict, List, Optional\nfrom dataclasses import dataclass, field\nimport json\n\n@dataclass\nclass APIConfig:\n    \"\"\"Configuration for AI model API integration\"\"\"\n    base_url: str = \"\"\n    api_key: str = \"\"\n    model_name: str = \"gpt-3.5-turbo\"\n    max_tokens: int = 2048\n    temperature: float = 0.7\n    timeout: int = 30\n    \n@dataclass\nclass LearningDomainConfig:\n    \"\"\"Configuration for each learning domain\"\"\"\n    domain_id: str\n    display_name: str\n    description: str\n    topics: List[str] = field(default_factory=list)\n    difficulty_levels: List[str] = field(default_factory=lambda: [\"beginner\", \"intermediate\", \"advanced\", \"expert\"])\n    assessment_types: List[str] = field(default_factory=lambda: [\"mcq\", \"subjective\", \"practical\", \"case_study\"])\n    \n@dataclass\nclass UserModelingConfig:\n    \"\"\"Configuration for TensorFlow user modeling\"\"\"\n    model_update_frequency: int = 10  # After every 10 interactions\n    feature_dimensions: int = 50\n    learning_rate: float = 0.001\n    batch_size: int = 32\n    epochs: int = 100\n    validation_split: float = 0.2\n    \n@dataclass\nclass ReinforcementLearningConfig:\n    \"\"\"Configuration for RL-based adaptive pathways\"\"\"\n    algorithm: str = \"PPO\"\n    exploration_rate: float = 0.1\n    learning_rate: float = 0.0003\n    gamma: float = 0.99  # Discount factor\n    update_frequency: int = 5\n    max_episode_length: int = 100\n    \nclass AdaptiveLearningConfig:\n    \"\"\"Main configuration class for the adaptive learning system\"\"\"\n    \n    def __init__(self):\n        self.api_config = APIConfig()\n        self.user_modeling = UserModelingConfig()\n        self.reinforcement_learning = ReinforcementLearningConfig()\n        \n        # Learning domains configuration\n        self.learning_domains = {\n            \"dsa_programming\": LearningDomainConfig(\n                domain_id=\"dsa_programming\",\n                display_name=\"Data Structures & Algorithms\",\n                description=\"Master coding interviews and competitive programming\",\n                topics=[\"arrays\", \"linked_lists\", \"trees\", \"graphs\", \"dynamic_programming\", \n                       \"greedy\", \"backtracking\", \"sorting\", \"searching\", \"system_design\"]\n            ),\n            \"upsc_preparation\": LearningDomainConfig(\n                domain_id=\"upsc_preparation\",\n                display_name=\"UPSC Civil Services\",\n                description=\"Comprehensive preparation for UPSC examinations\",\n                topics=[\"indian_polity\", \"geography\", \"history\", \"economics\", \"current_affairs\",\n                       \"ethics\", \"international_relations\", \"science_technology\", \"environment\", \"essay_writing\"]\n            ),\n            \"jee_preparation\": LearningDomainConfig(\n                domain_id=\"jee_preparation\", \n                display_name=\"JEE (Joint Entrance Examination)\",\n                description=\"Engineering entrance exam preparation\",\n                topics=[\"physics_mechanics\", \"physics_waves\", \"chemistry_organic\", \"chemistry_inorganic\",\n                       \"mathematics_calculus\", \"mathematics_algebra\", \"coordinate_geometry\", \"trigonometry\"]\n            ),\n            \"developer_skills\": LearningDomainConfig(\n                domain_id=\"developer_skills\",\n                display_name=\"Software Development\",\n                description=\"Modern software development skills and technologies\",\n                topics=[\"frontend_development\", \"backend_development\", \"mobile_development\", \"devops\",\n                       \"cloud_computing\", \"databases\", \"api_design\", \"testing\", \"security\", \"ai_ml_integration\"]\n            )\n        }\n        \n        # Assessment and adaptation settings\n        self.assessment_config = {\n            \"min_questions_per_topic\": 5,\n            \"max_questions_per_session\": 20,\n            \"adaptive_threshold\": 0.7,  # Accuracy threshold for difficulty adjustment\n            \"mastery_threshold\": 0.85,  # Threshold to consider topic mastered\n            \"time_weight\": 0.3,  # Weight given to time taken in scoring\n            \"difficulty_adjustment_rate\": 0.1\n        }\n        \n        # Gamification settings\n        self.gamification_config = {\n            \"points_per_correct_answer\": 10,\n            \"bonus_streak_multiplier\": 1.5,\n            \"daily_goal_bonus\": 50,\n            \"level_up_threshold\": 1000,\n            \"achievement_thresholds\": {\n                \"first_steps\": 1,\n                \"streak_warrior\": 7,\n                \"dedicated_learner\": 30,\n                \"topic_master\": 1,  # Master any topic\n                \"domain_expert\": 1   # Master any domain\n            }\n        }\n        \n    def update_api_config(self, base_url: str, api_key: str, model_name: str = None):\n        \"\"\"Update API configuration with user-provided credentials\"\"\"\n        self.api_config.base_url = base_url\n        self.api_config.api_key = api_key\n        if model_name:\n            self.api_config.model_name = model_name\n            \n    def get_domain_config(self, domain_id: str) -> Optional[LearningDomainConfig]:\n        \"\"\"Get configuration for a specific learning domain\"\"\"\n        return self.learning_domains.get(domain_id)\n        \n    def to_dict(self) -> Dict:\n        \"\"\"Convert configuration to dictionary for storage\"\"\"\n        return {\n            \"api_config\": self.api_config.__dict__,\n            \"user_modeling\": self.user_modeling.__dict__,\n            \"reinforcement_learning\": self.reinforcement_learning.__dict__,\n            \"assessment_config\": self.assessment_config,\n            \"gamification_config\": self.gamification_config\n        }\n        \n    @classmethod\n    def from_dict(cls, data: Dict):\n        \"\"\"Create configuration from dictionary\"\"\"\n        config = cls()\n        if \"api_config\" in data:\n            config.api_config = APIConfig(**data[\"api_config\"])\n        if \"user_modeling\" in data:\n            config.user_modeling = UserModelingConfig(**data[\"user_modeling\"])\n        if \"reinforcement_learning\" in data:\n            config.reinforcement_learning = ReinforcementLearningConfig(**data[\"reinforcement_learning\"])\n        if \"assessment_config\" in data:\n            config.assessment_config.update(data[\"assessment_config\"])\n        if \"gamification_config\" in data:\n            config.gamification_config.update(data[\"gamification_config\"])\n        return config\n\n# Global configuration instance\nconfig = AdaptiveLearningConfig()\n\n# Environment-based configuration loading\ndef load_config_from_env():\n    \"\"\"Load configuration from environment variables\"\"\"\n    if os.getenv(\"ADAPTIVE_LEARNING_API_URL\"):\n        config.update_api_config(\n            base_url=os.getenv(\"ADAPTIVE_LEARNING_API_URL\"),\n            api_key=os.getenv(\"ADAPTIVE_LEARNING_API_KEY\", \"\"),\n            model_name=os.getenv(\"ADAPTIVE_LEARNING_MODEL_NAME\", \"gpt-3.5-turbo\")\n        )\n\n# Auto-load on import\nload_config_from_env()\n"
    },
    {
      "type": "writeFile", 
      "path": "src/adaptive_learning/user_modeling.py",
      "ifExists": "overwrite",
      "description": "TensorFlow-based user modeling for adaptive learning",
      "content": "# User Modeling with TensorFlow for Adaptive Learning\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom typing import Dict, List, Tuple, Optional, Any\nimport json\nfrom datetime import datetime, timedelta\nfrom dataclasses import dataclass, asdict\nimport logging\nfrom .config import config\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass UserInteraction:\n    \"\"\"Represents a single user interaction with the learning system\"\"\"\n    timestamp: float\n    domain: str\n    topic: str\n    question_id: str\n    difficulty_level: str\n    response_time: float  # in seconds\n    is_correct: bool\n    hint_used: bool = False\n    attempts: int = 1\n    confidence_score: float = 0.5  # User's self-reported confidence (0-1)\n    \nclass UserModel:\n    \"\"\"TensorFlow-based user model for adaptive learning\"\"\"\n    \n    def __init__(self, user_id: str):\n        self.user_id = user_id\n        self.interactions_history: List[UserInteraction] = []\n        self.knowledge_state = {}  # Topic -> proficiency score (0-1)\n        self.learning_style_scores = {\n            \"visual\": 0.5,\n            \"auditory\": 0.5, \n            \"kinesthetic\": 0.5,\n            \"reading\": 0.5\n        }\n        self.cognitive_load_capacity = 0.7  # Estimated cognitive capacity (0-1)\n        self.motivation_level = 0.8  # Current motivation (0-1)\n        self.attention_span = 25.0  # Estimated attention span in minutes\n        \n        # TensorFlow models\n        self.knowledge_predictor = None\n        self.difficulty_recommender = None\n        self.engagement_predictor = None\n        \n        self._initialize_models()\n        \n    def _initialize_models(self):\n        \"\"\"Initialize TensorFlow models for user modeling\"\"\"\n        try:\n            # Knowledge state prediction model\n            self.knowledge_predictor = tf.keras.Sequential([\n                tf.keras.layers.Dense(64, activation='relu', input_shape=(config.user_modeling.feature_dimensions,)),\n                tf.keras.layers.Dropout(0.3),\n                tf.keras.layers.Dense(32, activation='relu'),\n                tf.keras.layers.Dense(16, activation='relu'),\n                tf.keras.layers.Dense(1, activation='sigmoid')  # Probability of knowing concept\n            ])\n            \n            self.knowledge_predictor.compile(\n                optimizer=tf.keras.optimizers.Adam(learning_rate=config.user_modeling.learning_rate),\n                loss='binary_crossentropy',\n                metrics=['accuracy']\n            )\n            \n            # Difficulty recommendation model (outputs difficulty level 0-1)\n            self.difficulty_recommender = tf.keras.Sequential([\n                tf.keras.layers.Dense(64, activation='relu', input_shape=(config.user_modeling.feature_dimensions,)),\n                tf.keras.layers.Dropout(0.2),\n                tf.keras.layers.Dense(32, activation='relu'),\n                tf.keras.layers.Dense(1, activation='sigmoid')  # Recommended difficulty (0-1)\n            ])\n            \n            self.difficulty_recommender.compile(\n                optimizer=tf.keras.optimizers.Adam(learning_rate=config.user_modeling.learning_rate),\n                loss='mse',\n                metrics=['mae']\n            )\n            \n            # Engagement prediction model\n            self.engagement_predictor = tf.keras.Sequential([\n                tf.keras.layers.Dense(32, activation='relu', input_shape=(20,)),  # Engagement features\n                tf.keras.layers.Dense(16, activation='relu'),\n                tf.keras.layers.Dense(1, activation='sigmoid')  # Predicted engagement (0-1)\n            ])\n            \n            self.engagement_predictor.compile(\n                optimizer=tf.keras.optimizers.Adam(learning_rate=config.user_modeling.learning_rate),\n                loss='binary_crossentropy',\n                metrics=['accuracy']\n            )\n            \n            logger.info(f\"Initialized TensorFlow models for user {self.user_id}\")\n            \n        except Exception as e:\n            logger.error(f\"Error initializing TensorFlow models: {str(e)}\")\n            # Fallback to simpler models or rule-based approach\n            self._initialize_fallback_models()\n            \n    def _initialize_fallback_models(self):\n        \"\"\"Initialize simpler fallback models if TensorFlow fails\"\"\"\n        logger.info(\"Using fallback rule-based models\")\n        self.use_tensorflow = False\n        \n    def add_interaction(self, interaction: UserInteraction):\n        \"\"\"Add a new user interaction and update the model\"\"\"\n        self.interactions_history.append(interaction)\n        \n        # Update knowledge state immediately\n        self._update_knowledge_state_immediate(interaction)\n        \n        # Update models periodically (every N interactions)\n        if len(self.interactions_history) % config.user_modeling.model_update_frequency == 0:\n            self._update_models()\n            \n    def _update_knowledge_state_immediate(self, interaction: UserInteraction):\n        \"\"\"Update knowledge state immediately after interaction using Bayesian update\"\"\"\n        topic = interaction.topic\n        current_prob = self.knowledge_state.get(topic, 0.5)\n        \n        # Evidence strength based on response characteristics\n        time_factor = min(1.0, 60.0 / max(1.0, interaction.response_time))  # Faster = more confident\n        confidence_factor = interaction.confidence_score\n        difficulty_factor = self._get_difficulty_multiplier(interaction.difficulty_level)\n        \n        evidence_strength = (time_factor + confidence_factor + difficulty_factor) / 3.0\n        \n        if interaction.is_correct:\n            # Positive evidence - increase knowledge probability\n            new_prob = current_prob + (1 - current_prob) * evidence_strength * 0.3\n        else:\n            # Negative evidence - decrease knowledge probability  \n            new_prob = current_prob * (1 - evidence_strength * 0.2)\n            \n        self.knowledge_state[topic] = max(0.01, min(0.99, new_prob))\n        \n    def _get_difficulty_multiplier(self, difficulty: str) -> float:\n        \"\"\"Get multiplier based on difficulty level\"\"\"\n        multipliers = {\n            \"beginner\": 0.3,\n            \"intermediate\": 0.6, \n            \"advanced\": 0.9,\n            \"expert\": 1.2\n        }\n        return multipliers.get(difficulty, 0.6)\n        \n    def _extract_features(self, interaction: UserInteraction = None, topic: str = None) -> np.ndarray:\n        \"\"\"Extract features for TensorFlow models\"\"\"\n        features = []\n        \n        # Recent performance features (last 10 interactions)\n        recent_interactions = self.interactions_history[-10:]\n        if len(recent_interactions) > 0:\n            features.extend([\n                np.mean([i.is_correct for i in recent_interactions]),\n                np.mean([i.response_time for i in recent_interactions]),\n                np.mean([i.attempts for i in recent_interactions]),\n                len([i for i in recent_interactions if i.hint_used]) / len(recent_interactions)\n            ])\n        else:\n            features.extend([0.5, 30.0, 1.0, 0.0])\n            \n        # Topic-specific features\n        if topic:\n            topic_interactions = [i for i in recent_interactions if i.topic == topic]\n            if topic_interactions:\n                features.extend([\n                    np.mean([i.is_correct for i in topic_interactions]),\n                    len(topic_interactions),\n                    self.knowledge_state.get(topic, 0.5)\n                ])\n            else:\n                features.extend([0.5, 0, 0.5])\n        else:\n            features.extend([0.5, 0, 0.5])\n            \n        # User characteristics\n        features.extend([\n            self.cognitive_load_capacity,\n            self.motivation_level,\n            self.attention_span / 60.0,  # Normalize to hours\n            *self.learning_style_scores.values()\n        ])\n        \n        # Time-based features\n        current_time = datetime.now()\n        time_features = [\n            current_time.hour / 24.0,  # Time of day\n            current_time.weekday() / 6.0,  # Day of week\n            len([i for i in recent_interactions if \n                 datetime.fromtimestamp(i.timestamp).date() == current_time.date()]) / 20.0  # Today's activity\n        ]\n        features.extend(time_features)\n        \n        # Pad or truncate to expected dimension\n        while len(features) < config.user_modeling.feature_dimensions:\n            features.append(0.0)\n        features = features[:config.user_modeling.feature_dimensions]\n        \n        return np.array(features, dtype=np.float32)\n        \n    def predict_knowledge_probability(self, topic: str) -> float:\n        \"\"\"Predict probability that user knows a topic\"\"\"\n        try:\n            if self.knowledge_predictor is not None and hasattr(self, 'use_tensorflow') and self.use_tensorflow != False:\n                features = self._extract_features(topic=topic).reshape(1, -1)\n                prediction = self.knowledge_predictor.predict(features, verbose=0)[0][0]\n                return float(prediction)\n        except Exception as e:\n            logger.warning(f\"TensorFlow prediction failed, using fallback: {str(e)}\")\n            \n        # Fallback to current knowledge state\n        return self.knowledge_state.get(topic, 0.5)\n        \n    def recommend_difficulty(self, topic: str, domain: str) -> str:\n        \"\"\"Recommend appropriate difficulty level for a topic\"\"\"\n        try:\n            if self.difficulty_recommender is not None and hasattr(self, 'use_tensorflow') and self.use_tensorflow != False:\n                features = self._extract_features(topic=topic).reshape(1, -1)\n                difficulty_score = self.difficulty_recommender.predict(features, verbose=0)[0][0]\n            else:\n                # Fallback calculation\n                knowledge_prob = self.predict_knowledge_probability(topic)\n                recent_performance = self._get_recent_topic_performance(topic)\n                difficulty_score = (knowledge_prob + recent_performance) / 2.0\n                \n            # Map score to difficulty level\n            if difficulty_score < 0.3:\n                return \"beginner\"\n            elif difficulty_score < 0.6:\n                return \"intermediate\" \n            elif difficulty_score < 0.85:\n                return \"advanced\"\n            else:\n                return \"expert\"\n                \n        except Exception as e:\n            logger.error(f\"Error in difficulty recommendation: {str(e)}\")\n            return \"intermediate\"  # Safe default\n            \n    def _get_recent_topic_performance(self, topic: str, days: int = 7) -> float:\n        \"\"\"Get recent performance for a specific topic\"\"\"\n        cutoff_time = datetime.now() - timedelta(days=days)\n        recent_interactions = [\n            i for i in self.interactions_history \n            if i.topic == topic and datetime.fromtimestamp(i.timestamp) > cutoff_time\n        ]\n        \n        if not recent_interactions:\n            return 0.5\n            \n        return np.mean([i.is_correct for i in recent_interactions])\n        \n    def predict_engagement(self, session_context: Dict) -> float:\n        \"\"\"Predict user engagement for current session\"\"\"\n        try:\n            engagement_features = self._extract_engagement_features(session_context)\n            \n            if self.engagement_predictor is not None and hasattr(self, 'use_tensorflow') and self.use_tensorflow != False:\n                prediction = self.engagement_predictor.predict(engagement_features.reshape(1, -1), verbose=0)[0][0]\n                return float(prediction)\n        except Exception as e:\n            logger.warning(f\"Engagement prediction failed: {str(e)}\")\n            \n        # Fallback engagement calculation\n        return self._calculate_engagement_heuristic(session_context)\n        \n    def _extract_engagement_features(self, session_context: Dict) -> np.ndarray:\n        \"\"\"Extract features for engagement prediction\"\"\"\n        features = []\n        \n        # Session characteristics\n        features.extend([\n            session_context.get('session_duration', 0) / 60.0,  # Duration in minutes\n            session_context.get('questions_answered', 0),\n            session_context.get('correct_answers', 0) / max(1, session_context.get('questions_answered', 1)),\n            session_context.get('hints_used', 0),\n            session_context.get('time_since_last_session', 24) / 24.0  # Hours since last session\n        ])\n        \n        # User state\n        features.extend([\n            self.motivation_level,\n            self.cognitive_load_capacity,\n            len(self.interactions_history) / 1000.0,  # Experience level\n            np.mean([i.is_correct for i in self.interactions_history[-20:]]) if self.interactions_history else 0.5\n        ])\n        \n        # Time and context\n        current_time = datetime.now()\n        features.extend([\n            current_time.hour / 24.0,\n            1.0 if current_time.weekday() < 5 else 0.0,  # Weekday vs weekend\n            session_context.get('difficulty_level_numeric', 0.5),\n            len(set([i.topic for i in self.interactions_history[-10:]])) / 10.0  # Topic variety\n        ])\n        \n        # Pad to 20 features\n        while len(features) < 20:\n            features.append(0.0)\n        features = features[:20]\n        \n        return np.array(features, dtype=np.float32)\n        \n    def _calculate_engagement_heuristic(self, session_context: Dict) -> float:\n        \"\"\"Fallback heuristic for engagement calculation\"\"\"\n        factors = []\n        \n        # Performance factor\n        recent_accuracy = session_context.get('correct_answers', 0) / max(1, session_context.get('questions_answered', 1))\n        factors.append(min(1.0, recent_accuracy * 1.5))  # Boost for good performance\n        \n        # Time factor\n        session_duration = session_context.get('session_duration', 0) / 60.0\n        optimal_duration = min(1.0, session_duration / self.attention_span)\n        factors.append(optimal_duration)\n        \n        # Motivation and capacity\n        factors.extend([self.motivation_level, self.cognitive_load_capacity])\n        \n        return np.mean(factors)\n        \n    def _update_models(self):\n        \"\"\"Update TensorFlow models with recent interaction data\"\"\"\n        if not hasattr(self, 'use_tensorflow') or self.use_tensorflow == False:\n            return\n            \n        if len(self.interactions_history) < 20:  # Need minimum data\n            return\n            \n        try:\n            # Prepare training data\n            X, y_knowledge, y_difficulty = self._prepare_training_data()\n            \n            if len(X) > 0:\n                # Update knowledge predictor\n                self.knowledge_predictor.fit(\n                    X, y_knowledge,\n                    epochs=config.user_modeling.epochs,\n                    batch_size=config.user_modeling.batch_size,\n                    validation_split=config.user_modeling.validation_split,\n                    verbose=0\n                )\n                \n                # Update difficulty recommender\n                self.difficulty_recommender.fit(\n                    X, y_difficulty,\n                    epochs=config.user_modeling.epochs,\n                    batch_size=config.user_modeling.batch_size,\n                    validation_split=config.user_modeling.validation_split,\n                    verbose=0\n                )\n                \n                logger.info(f\"Updated models for user {self.user_id} with {len(X)} samples\")\n                \n        except Exception as e:\n            logger.error(f\"Error updating models: {str(e)}\")\n            \n    def _prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Prepare training data from interaction history\"\"\"\n        X, y_knowledge, y_difficulty = [], [], []\n        \n        for i, interaction in enumerate(self.interactions_history[-100:]):  # Use last 100 interactions\n            features = self._extract_features(interaction, interaction.topic)\n            X.append(features)\n            \n            # Knowledge label (whether user got it right)\n            y_knowledge.append(float(interaction.is_correct))\n            \n            # Difficulty label (optimal difficulty based on performance)\n            optimal_difficulty = self._calculate_optimal_difficulty(interaction)\n            y_difficulty.append(optimal_difficulty)\n            \n        return np.array(X), np.array(y_knowledge), np.array(y_difficulty)\n        \n    def _calculate_optimal_difficulty(self, interaction: UserInteraction) -> float:\n        \"\"\"Calculate what the optimal difficulty should have been\"\"\"\n        # If user got it right quickly, could handle harder\n        if interaction.is_correct and interaction.response_time < 30:\n            return min(1.0, self._difficulty_to_numeric(interaction.difficulty_level) + 0.2)\n        # If user struggled, should be easier\n        elif not interaction.is_correct or interaction.response_time > 120:\n            return max(0.0, self._difficulty_to_numeric(interaction.difficulty_level) - 0.2)\n        else:\n            return self._difficulty_to_numeric(interaction.difficulty_level)\n            \n    def _difficulty_to_numeric(self, difficulty: str) -> float:\n        \"\"\"Convert difficulty string to numeric value\"\"\"\n        mapping = {\"beginner\": 0.25, \"intermediate\": 0.5, \"advanced\": 0.75, \"expert\": 1.0}\n        return mapping.get(difficulty, 0.5)\n        \n    def get_learning_analytics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive learning analytics for the user\"\"\"\n        if not self.interactions_history:\n            return {\"status\": \"no_data\", \"message\": \"No learning interactions recorded yet\"}\n            \n        recent_interactions = self.interactions_history[-20:]\n        \n        analytics = {\n            \"user_id\": self.user_id,\n            \"total_interactions\": len(self.interactions_history),\n            \"knowledge_state\": dict(self.knowledge_state),\n            \"overall_accuracy\": np.mean([i.is_correct for i in self.interactions_history]),\n            \"recent_accuracy\": np.mean([i.is_correct for i in recent_interactions]),\n            \"average_response_time\": np.mean([i.response_time for i in self.interactions_history]),\n            \"learning_style_scores\": dict(self.learning_style_scores),\n            \"cognitive_load_capacity\": self.cognitive_load_capacity,\n            \"motivation_level\": self.motivation_level,\n            \"attention_span_minutes\": self.attention_span,\n            \"topics_studied\": list(set([i.topic for i in self.interactions_history])),\n            \"domains_active\": list(set([i.domain for i in self.interactions_history])),\n            \"mastered_topics\": [topic for topic, prob in self.knowledge_state.items() if prob > 0.85],\n            \"struggling_topics\": [topic for topic, prob in self.knowledge_state.items() if prob < 0.3],\n            \"last_updated\": datetime.now().isoformat()\n        }\n        \n        # Performance trends\n        if len(self.interactions_history) >= 10:\n            first_half = self.interactions_history[:len(self.interactions_history)//2]\n            second_half = self.interactions_history[len(self.interactions_history)//2:]\n            \n            analytics[\"learning_trend\"] = {\n                \"early_accuracy\": np.mean([i.is_correct for i in first_half]),\n                \"recent_accuracy\": np.mean([i.is_correct for i in second_half]),\n                \"improvement\": np.mean([i.is_correct for i in second_half]) - np.mean([i.is_correct for i in first_half])\n            }\n            \n        return analytics\n        \n    def export_user_data(self) -> Dict[str, Any]:\n        \"\"\"Export all user data for privacy compliance\"\"\"\n        return {\n            \"user_model\": {\n                \"user_id\": self.user_id,\n                \"knowledge_state\": self.knowledge_state,\n                \"learning_style_scores\": self.learning_style_scores,\n                \"cognitive_load_capacity\": self.cognitive_load_capacity,\n                \"motivation_level\": self.motivation_level,\n                \"attention_span\": self.attention_span\n            },\n            \"interactions_history\": [asdict(interaction) for interaction in self.interactions_history],\n            \"analytics\": self.get_learning_analytics(),\n            \"exported_at\": datetime.now().isoformat()\n        }\n        \n    def clear_user_data(self):\n        \"\"\"Clear all user data for privacy compliance\"\"\"\n        self.interactions_history.clear()\n        self.knowledge_state.clear()\n        self.learning_style_scores = {\"visual\": 0.5, \"auditory\": 0.5, \"kinesthetic\": 0.5, \"reading\": 0.5}\n        self.cognitive_load_capacity = 0.7\n        self.motivation_level = 0.8\n        self.attention_span = 25.0\n        logger.info(f\"Cleared all data for user {self.user_id}\")\n"
    },
    {
      "type": "writeFile",
      "path": "src/adaptive_learning/reinforcement_learning.py",
      "ifExists": "overwrite", 
      "description": "Reinforcement Learning engine for adaptive pathways",
      "content": "# Reinforcement Learning for Adaptive Learning Pathways\nimport numpy as np\nimport random\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass, asdict\nfrom collections import defaultdict, deque\nimport json\nimport logging\nfrom .config import config\nfrom .user_modeling import UserModel, UserInteraction\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass LearningState:\n    \"\"\"Represents the current state of the learning environment\"\"\"\n    user_id: str\n    current_topic: str\n    current_domain: str\n    knowledge_level: float  # 0-1\n    difficulty_level: str\n    time_spent_today: float  # minutes\n    consecutive_correct: int\n    consecutive_incorrect: int\n    engagement_score: float  # 0-1\n    energy_level: float  # 0-1 (decreases with time, resets daily)\n    session_length: float  # minutes\n    \n    def to_feature_vector(self) -> np.ndarray:\n        \"\"\"Convert state to feature vector for RL algorithms\"\"\"\n        difficulty_encoding = {\n            \"beginner\": [1, 0, 0, 0],\n            \"intermediate\": [0, 1, 0, 0], \n            \"advanced\": [0, 0, 1, 0],\n            \"expert\": [0, 0, 0, 1]\n        }\n        \n        features = [\n            self.knowledge_level,\n            self.time_spent_today / 120.0,  # Normalize to 2 hours\n            self.consecutive_correct / 10.0,  # Normalize\n            self.consecutive_incorrect / 5.0,  # Normalize\n            self.engagement_score,\n            self.energy_level,\n            self.session_length / 60.0  # Normalize to 1 hour\n        ]\n        \n        # Add difficulty encoding\n        features.extend(difficulty_encoding.get(self.difficulty_level, [0, 1, 0, 0]))\n        \n        return np.array(features, dtype=np.float32)\n\n@dataclass\nclass LearningAction:\n    \"\"\"Represents an action the RL agent can take\"\"\"\n    action_type: str  # 'question', 'hint', 'explanation', 'break', 'review', 'advance'\n    topic: str\n    difficulty: str\n    content_type: str  # 'mcq', 'subjective', 'practical', 'case_study'\n    estimated_time: float  # minutes\n    \n    def to_index(self, action_space: List['LearningAction']) -> int:\n        \"\"\"Convert action to index in action space\"\"\"\n        try:\n            return action_space.index(self)\n        except ValueError:\n            return 0  # Default action\n            \nclass LearningEnvironment:\n    \"\"\"Reinforcement Learning environment for adaptive learning\"\"\"\n    \n    def __init__(self, user_model: UserModel, domain: str):\n        self.user_model = user_model\n        self.domain = domain\n        self.current_state = None\n        self.action_space = self._create_action_space()\n        self.state_history = deque(maxlen=100)\n        self.reward_history = deque(maxlen=100)\n        \n        # Environment parameters\n        self.max_session_length = 120.0  # minutes\n        self.optimal_engagement = 0.7\n        self.energy_decay_rate = 0.1  # per 10 minutes\n        \n    def _create_action_space(self) -> List[LearningAction]:\n        \"\"\"Create the action space for the RL agent\"\"\"\n        actions = []\n        \n        domain_config = config.get_domain_config(self.domain)\n        if not domain_config:\n            logger.warning(f\"No config found for domain {self.domain}\")\n            return [LearningAction(\"question\", \"general\", \"intermediate\", \"mcq\", 5.0)]\n            \n        # Generate actions for each topic and difficulty combination\n        for topic in domain_config.topics:\n            for difficulty in domain_config.difficulty_levels:\n                for content_type in domain_config.assessment_types:\n                    estimated_time = {\n                        \"mcq\": 2.0,\n                        \"subjective\": 8.0,\n                        \"practical\": 15.0,\n                        \"case_study\": 12.0\n                    }.get(content_type, 5.0)\n                    \n                    actions.extend([\n                        LearningAction(\"question\", topic, difficulty, content_type, estimated_time),\n                        LearningAction(\"explanation\", topic, difficulty, content_type, estimated_time * 0.7),\n                        LearningAction(\"hint\", topic, difficulty, content_type, estimated_time * 0.3)\n                    ])\n                    \n        # Add meta-actions\n        actions.extend([\n            LearningAction(\"break\", \"rest\", \"any\", \"break\", 5.0),\n            LearningAction(\"review\", \"mixed\", \"intermediate\", \"review\", 10.0),\n            LearningAction(\"advance\", \"next_topic\", \"any\", \"advance\", 0.0)\n        ])\n        \n        return actions\n        \n    def reset(self) -> LearningState:\n        \"\"\"Reset environment to initial state\"\"\"\n        # Get user's current knowledge state\n        knowledge_levels = list(self.user_model.knowledge_state.values())\n        avg_knowledge = np.mean(knowledge_levels) if knowledge_levels else 0.5\n        \n        self.current_state = LearningState(\n            user_id=self.user_model.user_id,\n            current_topic=self._select_initial_topic(),\n            current_domain=self.domain,\n            knowledge_level=avg_knowledge,\n            difficulty_level=\"intermediate\",\n            time_spent_today=0.0,\n            consecutive_correct=0,\n            consecutive_incorrect=0,\n            engagement_score=0.8,\n            energy_level=1.0,\n            session_length=0.0\n        )\n        \n        self.state_history.clear()\n        self.reward_history.clear()\n        \n        return self.current_state\n        \n    def _select_initial_topic(self) -> str:\n        \"\"\"Select initial topic based on user's knowledge state\"\"\"\n        domain_config = config.get_domain_config(self.domain)\n        if not domain_config:\n            return \"general\"\n            \n        # Find topic with lowest knowledge probability\n        topic_scores = {\n            topic: self.user_model.knowledge_state.get(topic, 0.5)\n            for topic in domain_config.topics\n        }\n        \n        return min(topic_scores, key=topic_scores.get)\n        \n    def step(self, action: LearningAction) -> Tuple[LearningState, float, bool, Dict]:\n        \"\"\"Execute action and return new state, reward, done flag, and info\"\"\"\n        prev_state = self.current_state\n        \n        # Simulate action execution\n        interaction_result = self._simulate_interaction(action)\n        \n        # Update state based on action result\n        self._update_state(action, interaction_result)\n        \n        # Calculate reward\n        reward = self._calculate_reward(prev_state, self.current_state, action, interaction_result)\n        \n        # Check if episode is done\n        done = self._is_episode_done()\n        \n        # Collect info\n        info = {\n            \"interaction_result\": interaction_result,\n            \"engagement_score\": self.current_state.engagement_score,\n            \"knowledge_gained\": interaction_result.get(\"knowledge_gained\", 0),\n            \"action_type\": action.action_type\n        }\n        \n        # Store history\n        self.state_history.append(asdict(prev_state))\n        self.reward_history.append(reward)\n        \n        return self.current_state, reward, done, info\n        \n    def _simulate_interaction(self, action: LearningAction) -> Dict[str, Any]:\n        \"\"\"Simulate user interaction with the given action\"\"\"\n        current_knowledge = self.user_model.knowledge_state.get(action.topic, 0.5)\n        \n        # Probability of success based on knowledge and difficulty\n        difficulty_factors = {\n            \"beginner\": 1.2,\n            \"intermediate\": 1.0,\n            \"advanced\": 0.8,\n            \"expert\": 0.6\n        }\n        \n        success_prob = current_knowledge * difficulty_factors.get(action.difficulty, 1.0)\n        success_prob = max(0.1, min(0.95, success_prob))  # Clamp between 10% and 95%\n        \n        # Simulate interaction\n        is_correct = random.random() < success_prob\n        \n        # Response time simulation (based on knowledge and difficulty)\n        if is_correct:\n            base_time = action.estimated_time * (1.5 - current_knowledge)\n        else:\n            base_time = action.estimated_time * (2.0 - current_knowledge * 0.5)\n            \n        response_time = max(5.0, base_time + random.gauss(0, base_time * 0.3))\n        \n        # Knowledge gain calculation\n        if is_correct:\n            knowledge_gain = (1 - current_knowledge) * 0.1 * difficulty_factors.get(action.difficulty, 1.0)\n        else:\n            knowledge_gain = -current_knowledge * 0.05  # Small penalty for wrong answers\n            \n        return {\n            \"is_correct\": is_correct,\n            \"response_time\": response_time,\n            \"knowledge_gained\": knowledge_gain,\n            \"engagement_impact\": self._calculate_engagement_impact(action, is_correct),\n            \"energy_cost\": self._calculate_energy_cost(action, response_time)\n        }\n        \n    def _calculate_engagement_impact(self, action: LearningAction, is_correct: bool) -> float:\n        \"\"\"Calculate impact on engagement based on action and result\"\"\"\n        base_impact = {\n            \"question\": 0.1 if is_correct else -0.05,\n            \"hint\": 0.02,\n            \"explanation\": 0.05,\n            \"break\": 0.15,\n            \"review\": -0.02,\n            \"advance\": 0.08\n        }.get(action.action_type, 0)\n        \n        # Difficulty impact\n        difficulty_impact = {\n            \"beginner\": -0.02,  # Might be too easy\n            \"intermediate\": 0,\n            \"advanced\": 0.03,\n            \"expert\": 0.05 if is_correct else -0.10  # High reward if correct, penalty if not\n        }.get(action.difficulty, 0)\n        \n        return base_impact + difficulty_impact\n        \n    def _calculate_energy_cost(self, action: LearningAction, response_time: float) -> float:\n        \"\"\"Calculate energy cost of an action\"\"\"\n        base_costs = {\n            \"question\": 0.05,\n            \"hint\": 0.01,\n            \"explanation\": 0.02,\n            \"break\": -0.2,  # Restores energy\n            \"review\": 0.03,\n            \"advance\": 0.01\n        }\n        \n        cost = base_costs.get(action.action_type, 0.05)\n        \n        # Additional cost for longer response times (indicates cognitive load)\n        if response_time > action.estimated_time * 1.5:\n            cost += 0.02\n            \n        return cost\n        \n    def _update_state(self, action: LearningAction, interaction_result: Dict):\n        \"\"\"Update current state based on action and its result\"\"\"\n        # Update knowledge level for the topic\n        if action.topic in self.user_model.knowledge_state:\n            old_knowledge = self.user_model.knowledge_state[action.topic]\n            new_knowledge = old_knowledge + interaction_result[\"knowledge_gained\"]\n            self.user_model.knowledge_state[action.topic] = max(0.01, min(0.99, new_knowledge))\n        else:\n            initial_knowledge = 0.5 + interaction_result[\"knowledge_gained\"]\n            self.user_model.knowledge_state[action.topic] = max(0.01, min(0.99, initial_knowledge))\n            \n        # Update current state\n        self.current_state.current_topic = action.topic\n        self.current_state.knowledge_level = self.user_model.knowledge_state[action.topic]\n        self.current_state.session_length += action.estimated_time\n        self.current_state.time_spent_today += action.estimated_time\n        \n        # Update streaks\n        if interaction_result[\"is_correct\"]:\n            self.current_state.consecutive_correct += 1\n            self.current_state.consecutive_incorrect = 0\n        else:\n            self.current_state.consecutive_correct = 0\n            self.current_state.consecutive_incorrect += 1\n            \n        # Update engagement\n        engagement_change = interaction_result[\"engagement_impact\"]\n        self.current_state.engagement_score = max(0.1, min(1.0, \n            self.current_state.engagement_score + engagement_change))\n            \n        # Update energy\n        energy_cost = interaction_result[\"energy_cost\"]\n        self.current_state.energy_level = max(0.1, min(1.0,\n            self.current_state.energy_level - energy_cost))\n            \n    def _calculate_reward(self, prev_state: LearningState, new_state: LearningState, \n                        action: LearningAction, interaction_result: Dict) -> float:\n        \"\"\"Calculate reward for the RL agent\"\"\"\n        reward = 0.0\n        \n        # Learning reward (primary objective)\n        knowledge_gain = interaction_result[\"knowledge_gained\"]\n        reward += knowledge_gain * 10.0  # Scale up knowledge gains\n        \n        # Engagement reward\n        engagement_diff = new_state.engagement_score - prev_state.engagement_score\n        reward += engagement_diff * 5.0\n        \n        # Energy management reward\n        if new_state.energy_level > 0.3:  # Reward maintaining good energy\n            reward += 0.5\n        elif new_state.energy_level < 0.2:  # Penalty for exhaustion\n            reward -= 1.0\n            \n        # Time efficiency reward\n        if action.action_type == \"question\" and interaction_result[\"is_correct\"]:\n            time_efficiency = action.estimated_time / interaction_result[\"response_time\"]\n            reward += min(2.0, time_efficiency)  # Reward faster correct answers\n            \n        # Difficulty appropriateness reward\n        if interaction_result[\"is_correct\"] and action.difficulty in [\"advanced\", \"expert\"]:\n            reward += 1.0  # Bonus for handling difficult content\n        elif not interaction_result[\"is_correct\"] and action.difficulty == \"beginner\":\n            reward -= 0.5  # Penalty for struggling with easy content\n            \n        # Session length management\n        if new_state.session_length > self.max_session_length:\n            reward -= 2.0  # Penalty for overly long sessions\n            \n        # Break timing reward\n        if action.action_type == \"break\" and prev_state.energy_level < 0.4:\n            reward += 1.5  # Good timing for breaks\n            \n        return reward\n        \n    def _is_episode_done(self) -> bool:\n        \"\"\"Check if the learning episode should end\"\"\"\n        return (\n            self.current_state.session_length >= self.max_session_length or\n            self.current_state.energy_level <= 0.1 or\n            self.current_state.engagement_score <= 0.2 or\n            self.current_state.consecutive_incorrect >= 5\n        )\n        \nclass AdaptivePathwayAgent:\n    \"\"\"RL Agent for learning pathway optimization\"\"\"\n    \n    def __init__(self, environment: LearningEnvironment):\n        self.env = environment\n        self.q_table = defaultdict(lambda: defaultdict(float))  # State -> Action -> Q-value\n        self.learning_rate = config.reinforcement_learning.learning_rate\n        self.discount_factor = config.reinforcement_learning.gamma\n        self.exploration_rate = config.reinforcement_learning.exploration_rate\n        self.exploration_decay = 0.995\n        self.min_exploration_rate = 0.01\n        \n        # Experience replay buffer\n        self.experience_buffer = deque(maxlen=10000)\n        \n    def get_state_key(self, state: LearningState) -> str:\n        \"\"\"Convert state to string key for Q-table\"\"\"\n        # Discretize continuous values for Q-table\n        knowledge_bucket = int(state.knowledge_level * 10)  # 0-10\n        engagement_bucket = int(state.engagement_score * 5)  # 0-5\n        energy_bucket = int(state.energy_level * 5)  # 0-5\n        \n        return f\"{state.current_topic}_{knowledge_bucket}_{engagement_bucket}_{energy_bucket}_{state.difficulty_level}\"\n        \n    def select_action(self, state: LearningState) -> LearningAction:\n        \"\"\"Select action using epsilon-greedy policy\"\"\"\n        state_key = self.get_state_key(state)\n        \n        # Exploration vs exploitation\n        if random.random() < self.exploration_rate:\n            # Random action (exploration)\n            action = random.choice(self.env.action_space)\n        else:\n            # Best known action (exploitation)\n            action_values = self.q_table[state_key]\n            if not action_values:\n                # No learned values, choose random\n                action = random.choice(self.env.action_space)\n            else:\n                # Choose best action\n                best_action_idx = max(action_values, key=action_values.get)\n                action = self.env.action_space[best_action_idx]\n                \n        return action\n        \n    def update_q_value(self, state: LearningState, action: LearningAction, \n                      reward: float, next_state: LearningState, done: bool):\n        \"\"\"Update Q-value using Q-learning update rule\"\"\"\n        state_key = self.get_state_key(state)\n        next_state_key = self.get_state_key(next_state)\n        action_idx = action.to_index(self.env.action_space)\n        \n        # Current Q-value\n        current_q = self.q_table[state_key][action_idx]\n        \n        # Next state max Q-value\n        if done:\n            next_max_q = 0\n        else:\n            next_q_values = self.q_table[next_state_key]\n            next_max_q = max(next_q_values.values()) if next_q_values else 0\n            \n        # Q-learning update\n        new_q = current_q + self.learning_rate * (\n            reward + self.discount_factor * next_max_q - current_q\n        )\n        \n        self.q_table[state_key][action_idx] = new_q\n        \n        # Store experience for potential replay\n        self.experience_buffer.append({\n            \"state\": state,\n            \"action\": action,\n            \"reward\": reward,\n            \"next_state\": next_state,\n            \"done\": done\n        })\n        \n    def train_episode(self) -> Dict[str, float]:\n        \"\"\"Train the agent for one episode\"\"\"\n        state = self.env.reset()\n        total_reward = 0.0\n        steps = 0\n        \n        while True:\n            action = self.select_action(state)\n            next_state, reward, done, info = self.env.step(action)\n            \n            self.update_q_value(state, action, reward, next_state, done)\n            \n            total_reward += reward\n            steps += 1\n            state = next_state\n            \n            if done:\n                break\n                \n        # Decay exploration rate\n        self.exploration_rate = max(\n            self.min_exploration_rate,\n            self.exploration_rate * self.exploration_decay\n        )\n        \n        return {\n            \"total_reward\": total_reward,\n            \"steps\": steps,\n            \"final_engagement\": state.engagement_score,\n            \"knowledge_gained\": state.knowledge_level,\n            \"exploration_rate\": self.exploration_rate\n        }\n        \n    def get_recommended_action(self, current_state: LearningState) -> LearningAction:\n        \"\"\"Get recommended action for current state (no exploration)\"\"\"\n        state_key = self.get_state_key(current_state)\n        action_values = self.q_table[state_key]\n        \n        if not action_values:\n            # No learned policy, use heuristic\n            return self._heuristic_action_selection(current_state)\n            \n        best_action_idx = max(action_values, key=action_values.get)\n        return self.env.action_space[best_action_idx]\n        \n    def _heuristic_action_selection(self, state: LearningState) -> LearningAction:\n        \"\"\"Fallback heuristic for action selection when no policy is learned\"\"\"\n        # Simple heuristics based on current state\n        \n        if state.energy_level < 0.3:\n            # Suggest break if energy is low\n            return next((a for a in self.env.action_space if a.action_type == \"break\"), \n                       self.env.action_space[0])\n                       \n        if state.engagement_score < 0.4:\n            # Suggest easier content if engagement is low\n            easier_actions = [a for a in self.env.action_space \n                            if a.difficulty in [\"beginner\", \"intermediate\"] and a.action_type == \"question\"]\n            return random.choice(easier_actions) if easier_actions else self.env.action_space[0]\n            \n        if state.consecutive_incorrect >= 3:\n            # Suggest hint or explanation if struggling\n            help_actions = [a for a in self.env.action_space \n                          if a.action_type in [\"hint\", \"explanation\"] and a.topic == state.current_topic]\n            return random.choice(help_actions) if help_actions else self.env.action_space[0]\n            \n        if state.knowledge_level > 0.8:\n            # Suggest harder content if topic is mastered\n            harder_actions = [a for a in self.env.action_space\n                            if a.difficulty in [\"advanced\", \"expert\"] and a.action_type == \"question\"]\n            return random.choice(harder_actions) if harder_actions else self.env.action_space[0]\n            \n        # Default: intermediate difficulty question on current topic\n        default_actions = [a for a in self.env.action_space\n                         if a.difficulty == \"intermediate\" and a.action_type == \"question\"]\n        return random.choice(default_actions) if default_actions else self.env.action_space[0]\n        \n    def save_model(self) -> Dict[str, Any]:\n        \"\"\"Save the learned Q-table and agent parameters\"\"\"\n        return {\n            \"q_table\": dict(self.q_table),\n            \"exploration_rate\": self.exploration_rate,\n            \"learning_rate\": self.learning_rate,\n            \"discount_factor\": self.discount_factor\n        }\n        \n    def load_model(self, model_data: Dict[str, Any]):\n        \"\"\"Load a saved Q-table and agent parameters\"\"\"\n        if \"q_table\" in model_data:\n            self.q_table = defaultdict(lambda: defaultdict(float), model_data[\"q_table\"])\n        if \"exploration_rate\" in model_data:\n            self.exploration_rate = model_data[\"exploration_rate\"]\n        if \"learning_rate\" in model_data:\n            self.learning_rate = model_data[\"learning_rate\"]\n        if \"discount_factor\" in model_data:\n            self.discount_factor = model_data[\"discount_factor\"]\n"
    },
    {
      "type": "writeFile",
      "path": "src/adaptive_learning/content_generation.py", 
      "ifExists": "overwrite",
      "description": "AI-powered content generation and question creation system",
      "content": "# AI-Powered Content Generation for Adaptive Learning\nimport asyncio\nimport aiohttp\nimport json\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom dataclasses import dataclass, asdict\nimport random\nfrom datetime import datetime\nimport logging\nfrom .config import config\nfrom .user_modeling import UserModel\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass Question:\n    \"\"\"Represents a generated question\"\"\"\n    id: str\n    domain: str\n    topic: str\n    difficulty: str\n    question_type: str  # 'mcq', 'subjective', 'practical', 'case_study'\n    question_text: str\n    options: List[str] = None  # For MCQ\n    correct_answer: str = \"\"\n    explanation: str = \"\"\n    hints: List[str] = None\n    estimated_time: float = 5.0  # minutes\n    tags: List[str] = None\n    created_at: float = None\n    \n    def __post_init__(self):\n        if self.created_at is None:\n            self.created_at = datetime.now().timestamp()\n        if self.options is None:\n            self.options = []\n        if self.hints is None:\n            self.hints = []\n        if self.tags is None:\n            self.tags = []\n            \n@dataclass\nclass LearningContent:\n    \"\"\"Represents learning material/explanation content\"\"\"\n    id: str\n    domain: str\n    topic: str\n    content_type: str  # 'explanation', 'example', 'tutorial', 'summary'\n    title: str\n    content: str\n    difficulty: str\n    estimated_read_time: float  # minutes\n    prerequisites: List[str] = None\n    learning_objectives: List[str] = None\n    created_at: float = None\n    \n    def __post_init__(self):\n        if self.created_at is None:\n            self.created_at = datetime.now().timestamp()\n        if self.prerequisites is None:\n            self.prerequisites = []\n        if self.learning_objectives is None:\n            self.learning_objectives = []\n\nclass AIContentGenerator:\n    \"\"\"AI-powered content generation system\"\"\"\n    \n    def __init__(self):\n        self.api_config = config.api_config\n        self.session = None\n        self.question_templates = self._load_question_templates()\n        self.content_templates = self._load_content_templates()\n        \n    async def __aenter__(self):\n        self.session = aiohttp.ClientSession()\n        return self\n        \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        if self.session:\n            await self.session.close()\n            \n    def _load_question_templates(self) -> Dict[str, Dict[str, str]]:\n        \"\"\"Load question templates for different domains and types\"\"\"\n        return {\n            \"dsa_programming\": {\n                \"mcq\": \"Create a multiple-choice question about {topic} at {difficulty} level. Include 4 options with one correct answer and explanation.\",\n                \"practical\": \"Create a coding problem about {topic} at {difficulty} level. Include problem statement, input/output format, constraints, and sample test cases.\",\n                \"subjective\": \"Create a theoretical question about {topic} at {difficulty} level that requires explanation of concepts.\"\n            },\n            \"upsc_preparation\": {\n                \"mcq\": \"Create a UPSC-style multiple-choice question on {topic} at {difficulty} level. Include 4 options and detailed explanation.\",\n                \"subjective\": \"Create a UPSC mains-style question on {topic} at {difficulty} level for essay writing or descriptive answer.\",\n                \"case_study\": \"Create a case study question on {topic} at {difficulty} level with scenario analysis.\"\n            },\n            \"jee_preparation\": {\n                \"mcq\": \"Create a JEE-style multiple-choice question on {topic} at {difficulty} level with 4 options and step-by-step solution.\",\n                \"subjective\": \"Create a JEE subjective question on {topic} at {difficulty} level that requires mathematical derivation or proof.\",\n                \"practical\": \"Create a JEE numerical problem on {topic} at {difficulty} level with given data and required calculations.\"\n            },\n            \"developer_skills\": {\n                \"mcq\": \"Create a technical interview question about {topic} at {difficulty} level with multiple-choice options.\",\n                \"practical\": \"Create a hands-on coding/implementation task for {topic} at {difficulty} level.\",\n                \"case_study\": \"Create a system design or architecture question about {topic} at {difficulty} level.\"\n            }\n        }\n        \n    def _load_content_templates(self) -> Dict[str, str]:\n        \"\"\"Load content generation templates\"\"\"\n        return {\n            \"explanation\": \"Provide a clear and comprehensive explanation of {topic} at {difficulty} level. Include key concepts, examples, and practical applications.\",\n            \"tutorial\": \"Create a step-by-step tutorial on {topic} at {difficulty} level. Include practical examples and hands-on exercises.\",\n            \"summary\": \"Create a concise summary of {topic} at {difficulty} level covering the most important points.\",\n            \"example\": \"Provide detailed examples and use cases for {topic} at {difficulty} level with explanations.\"\n        }\n        \n    async def generate_question(self, domain: str, topic: str, difficulty: str, \n                              question_type: str, user_context: Optional[Dict] = None) -> Question:\n        \"\"\"Generate a question using AI\"\"\"\n        try:\n            # Get appropriate template\n            domain_templates = self.question_templates.get(domain, {})\n            template = domain_templates.get(question_type, \n                \"Create a {question_type} question about {topic} at {difficulty} level.\")\n                \n            # Build prompt with context\n            prompt = self._build_question_prompt(template, domain, topic, difficulty, question_type, user_context)\n            \n            # Generate content using AI\n            response = await self._call_ai_api(prompt)\n            \n            # Parse response into Question object\n            question = self._parse_question_response(response, domain, topic, difficulty, question_type)\n            \n            return question\n            \n        except Exception as e:\n            logger.error(f\"Error generating question: {str(e)}\")\n            # Return fallback question\n            return self._create_fallback_question(domain, topic, difficulty, question_type)\n            \n    def _build_question_prompt(self, template: str, domain: str, topic: str, \n                             difficulty: str, question_type: str, user_context: Optional[Dict]) -> str:\n        \"\"\"Build comprehensive prompt for question generation\"\"\"\n        prompt = template.format(\n            topic=topic,\n            difficulty=difficulty,\n            question_type=question_type\n        )\n        \n        # Add domain-specific context\n        domain_context = self._get_domain_context(domain)\n        prompt += f\"\\n\\nDomain Context: {domain_context}\"\n        \n        # Add user context if available\n        if user_context:\n            knowledge_level = user_context.get('knowledge_level', 0.5)\n            learning_style = user_context.get('preferred_learning_style', 'mixed')\n            recent_topics = user_context.get('recent_topics', [])\n            \n            prompt += f\"\\n\\nUser Context:\"\n            prompt += f\"\\n- Knowledge Level: {knowledge_level:.2f} (0.0-1.0)\"\n            prompt += f\"\\n- Learning Style: {learning_style}\"\n            prompt += f\"\\n- Recent Topics: {', '.join(recent_topics[-5:])}\"\n            \n        # Add output format specification\n        prompt += self._get_output_format_spec(question_type)\n        \n        return prompt\n        \n    def _get_domain_context(self, domain: str) -> str:\n        \"\"\"Get domain-specific context for better question generation\"\"\"\n        contexts = {\n            \"dsa_programming\": \"Focus on algorithms, data structures, time/space complexity, and coding best practices. Questions should be relevant for technical interviews and competitive programming.\",\n            \"upsc_preparation\": \"Focus on Indian administration, governance, current affairs, and analytical thinking. Questions should match UPSC exam patterns and difficulty levels.\",\n            \"jee_preparation\": \"Focus on Physics, Chemistry, and Mathematics concepts. Questions should match JEE exam patterns with appropriate difficulty and numerical problems.\",\n            \"developer_skills\": \"Focus on modern software development practices, technologies, and real-world application scenarios. Questions should be relevant for software development roles.\"\n        }\n        return contexts.get(domain, \"Generate educationally sound questions appropriate for the learning context.\")\n        \n    def _get_output_format_spec(self, question_type: str) -> str:\n        \"\"\"Get output format specification for structured responses\"\"\"\n        if question_type == \"mcq\":\n            return \"\"\"\n            \nOutput Format (JSON):\n{\n  \"question\": \"Question text here\",\n  \"options\": [\"A) Option 1\", \"B) Option 2\", \"C) Option 3\", \"D) Option 4\"],\n  \"correct_answer\": \"A\",\n  \"explanation\": \"Detailed explanation of the correct answer\",\n  \"hints\": [\"Hint 1\", \"Hint 2\"],\n  \"estimated_time\": 3.0\n}\n\"\"\"\n        elif question_type == \"practical\":\n            return \"\"\"\n            \nOutput Format (JSON):\n{\n  \"question\": \"Problem statement\",\n  \"input_format\": \"Description of input format\",\n  \"output_format\": \"Description of expected output\",\n  \"constraints\": \"Problem constraints\",\n  \"sample_input\": \"Sample input\",\n  \"sample_output\": \"Sample output\",\n  \"explanation\": \"Approach and solution explanation\",\n  \"hints\": [\"Hint 1\", \"Hint 2\"],\n  \"estimated_time\": 15.0\n}\n\"\"\"\n        else:\n            return \"\"\"\n            \nOutput Format (JSON):\n{\n  \"question\": \"Question text\",\n  \"answer_guidelines\": \"Guidelines for answering\",\n  \"key_points\": [\"Point 1\", \"Point 2\", \"Point 3\"],\n  \"explanation\": \"Sample answer or explanation\",\n  \"estimated_time\": 8.0\n}\n\"\"\"\n            \n    async def _call_ai_api(self, prompt: str) -> str:\n        \"\"\"Call the configured AI API\"\"\"\n        if not self.api_config.base_url or not self.api_config.api_key:\n            raise Exception(\"AI API not configured\")\n            \n        headers = {\n            \"Authorization\": f\"Bearer {self.api_config.api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n        \n        payload = {\n            \"model\": self.api_config.model_name,\n            \"messages\": [\n                {\n                    \"role\": \"system\", \n                    \"content\": \"You are an expert educational content creator. Generate high-quality, pedagogically sound learning materials.\"\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }\n            ],\n            \"max_tokens\": self.api_config.max_tokens,\n            \"temperature\": self.api_config.temperature\n        }\n        \n        timeout = aiohttp.ClientTimeout(total=self.api_config.timeout)\n        \n        async with self.session.post(\n            f\"{self.api_config.base_url}/chat/completions\",\n            json=payload,\n            headers=headers,\n            timeout=timeout\n        ) as response:\n            if response.status == 200:\n                data = await response.json()\n                return data[\"choices\"][0][\"message\"][\"content\"]\n            else:\n                error_text = await response.text()\n                raise Exception(f\"API call failed: {response.status} - {error_text}\")\n                \n    def _parse_question_response(self, response: str, domain: str, topic: str, \n                               difficulty: str, question_type: str) -> Question:\n        \"\"\"Parse AI response into Question object\"\"\"\n        try:\n            # Try to extract JSON from response\n            json_start = response.find('{')\n            json_end = response.rfind('}') + 1\n            \n            if json_start != -1 and json_end != -1:\n                json_str = response[json_start:json_end]\n                data = json.loads(json_str)\n            else:\n                # Fallback: try to parse the entire response as JSON\n                data = json.loads(response)\n                \n            question_id = f\"{domain}_{topic}_{difficulty}_{int(datetime.now().timestamp())}\"\n            \n            # Handle different response formats\n            if question_type == \"mcq\":\n                return Question(\n                    id=question_id,\n                    domain=domain,\n                    topic=topic,\n                    difficulty=difficulty,\n                    question_type=question_type,\n                    question_text=data.get(\"question\", \"\"),\n                    options=data.get(\"options\", []),\n                    correct_answer=data.get(\"correct_answer\", \"\"),\n                    explanation=data.get(\"explanation\", \"\"),\n                    hints=data.get(\"hints\", []),\n                    estimated_time=data.get(\"estimated_time\", 5.0)\n                )\n            elif question_type == \"practical\":\n                # Combine practical question parts\n                question_text = data.get(\"question\", \"\")\n                if data.get(\"input_format\"):\n                    question_text += f\"\\n\\nInput Format: {data['input_format']}\"\n                if data.get(\"output_format\"):\n                    question_text += f\"\\nOutput Format: {data['output_format']}\"\n                if data.get(\"constraints\"):\n                    question_text += f\"\\nConstraints: {data['constraints']}\"\n                if data.get(\"sample_input\") and data.get(\"sample_output\"):\n                    question_text += f\"\\n\\nSample Input:\\n{data['sample_input']}\"\n                    question_text += f\"\\nSample Output:\\n{data['sample_output']}\"\n                    \n                return Question(\n                    id=question_id,\n                    domain=domain,\n                    topic=topic,\n                    difficulty=difficulty,\n                    question_type=question_type,\n                    question_text=question_text,\n                    explanation=data.get(\"explanation\", \"\"),\n                    hints=data.get(\"hints\", []),\n                    estimated_time=data.get(\"estimated_time\", 15.0)\n                )\n            else:\n                # Subjective/case study\n                question_text = data.get(\"question\", \"\")\n                if data.get(\"answer_guidelines\"):\n                    question_text += f\"\\n\\nAnswer Guidelines: {data['answer_guidelines']}\"\n                    \n                return Question(\n                    id=question_id,\n                    domain=domain,\n                    topic=topic,\n                    difficulty=difficulty,\n                    question_type=question_type,\n                    question_text=question_text,\n                    correct_answer=data.get(\"explanation\", \"\"),\n                    explanation=data.get(\"explanation\", \"\"),\n                    hints=data.get(\"key_points\", []),\n                    estimated_time=data.get(\"estimated_time\", 8.0)\n                )\n                \n        except Exception as e:\n            logger.error(f\"Error parsing question response: {str(e)}\")\n            # Return fallback question with raw response\n            return self._create_fallback_question(domain, topic, difficulty, question_type, response)\n            \n    def _create_fallback_question(self, domain: str, topic: str, difficulty: str, \n                                question_type: str, raw_response: str = None) -> Question:\n        \"\"\"Create a fallback question when AI generation fails\"\"\"\n        question_id = f\"fallback_{domain}_{topic}_{int(datetime.now().timestamp())}\"\n        \n        fallback_questions = {\n            \"dsa_programming\": {\n                \"arrays\": \"What is the time complexity of accessing an element in an array by index?\",\n                \"trees\": \"What is the difference between a binary tree and a binary search tree?\",\n                \"graphs\": \"Explain the difference between DFS and BFS traversal algorithms.\"\n            },\n            \"upsc_preparation\": {\n                \"indian_polity\": \"Explain the concept of separation of powers in the Indian Constitution.\",\n                \"geography\": \"Discuss the factors affecting the climate of India.\",\n                \"current_affairs\": \"Analyze the significance of recent policy changes in governance.\"\n            },\n            \"jee_preparation\": {\n                \"physics_mechanics\": \"A ball is thrown vertically upward. Derive the expression for maximum height reached.\",\n                \"chemistry_organic\": \"Explain the mechanism of nucleophilic substitution reactions.\",\n                \"mathematics_calculus\": \"Find the derivative of f(x) = x^3 + 2x^2 - 5x + 1.\"\n            },\n            \"developer_skills\": {\n                \"frontend_development\": \"What are the key differences between React and Angular frameworks?\",\n                \"backend_development\": \"Explain the concepts of RESTful API design principles.\",\n                \"databases\": \"Compare SQL and NoSQL databases with examples.\"\n            }\n        }\n        \n        domain_questions = fallback_questions.get(domain, {})\n        question_text = domain_questions.get(topic, f\"Explain the key concepts of {topic} at {difficulty} level.\")\n        \n        if raw_response:\n            question_text += f\"\\n\\n(Note: AI-generated content may need review)\\n{raw_response[:500]}\"\n            \n        return Question(\n            id=question_id,\n            domain=domain,\n            topic=topic,\n            difficulty=difficulty,\n            question_type=question_type,\n            question_text=question_text,\n            explanation=\"Please review the question content and provide appropriate explanation.\",\n            estimated_time=5.0\n        )\n        \n    async def generate_learning_content(self, domain: str, topic: str, difficulty: str,\n                                      content_type: str, user_context: Optional[Dict] = None) -> LearningContent:\n        \"\"\"Generate learning content (explanations, tutorials, etc.)\"\"\"\n        try:\n            template = self.content_templates.get(content_type, \n                \"Create {content_type} content about {topic} at {difficulty} level.\")\n                \n            prompt = template.format(\n                topic=topic,\n                difficulty=difficulty,\n                content_type=content_type\n            )\n            \n            # Add context\n            prompt += f\"\\n\\nDomain: {domain}\"\n            prompt += f\"\\nTarget Audience: Learners at {difficulty} level\"\n            \n            if user_context:\n                prompt += f\"\\nUser Context: {json.dumps(user_context, indent=2)}\"\n                \n            prompt += \"\\n\\nGenerate comprehensive, well-structured learning content with clear explanations and practical examples.\"\n            \n            response = await self._call_ai_api(prompt)\n            \n            content_id = f\"content_{domain}_{topic}_{content_type}_{int(datetime.now().timestamp())}\"\n            \n            return LearningContent(\n                id=content_id,\n                domain=domain,\n                topic=topic,\n                content_type=content_type,\n                title=f\"{topic.replace('_', ' ').title()} - {content_type.title()}\",\n                content=response,\n                difficulty=difficulty,\n                estimated_read_time=len(response.split()) / 200.0  # Assume 200 WPM reading speed\n            )\n            \n        except Exception as e:\n            logger.error(f\"Error generating learning content: {str(e)}\")\n            return self._create_fallback_content(domain, topic, difficulty, content_type)\n            \n    def _create_fallback_content(self, domain: str, topic: str, difficulty: str, \n                               content_type: str) -> LearningContent:\n        \"\"\"Create fallback content when AI generation fails\"\"\"\n        content_id = f\"fallback_content_{domain}_{topic}_{int(datetime.now().timestamp())}\"\n        \n        fallback_content = f\"\"\"# {topic.replace('_', ' ').title()}\n\nThis is a placeholder for {content_type} content about {topic} at {difficulty} level.\n\n## Key Points to Cover:\n- Fundamental concepts and definitions\n- Practical applications and examples\n- Common challenges and solutions\n- Best practices and recommendations\n\n*Note: This content needs to be reviewed and completed by an expert.*\n\"\"\"\n        \n        return LearningContent(\n            id=content_id,\n            domain=domain,\n            topic=topic,\n            content_type=content_type,\n            title=f\"{topic.replace('_', ' ').title()} - {content_type.title()}\",\n            content=fallback_content,\n            difficulty=difficulty,\n            estimated_read_time=2.0\n        )\n        \n    async def generate_personalized_study_plan(self, user_model: UserModel, \n                                             domain: str, duration_days: int = 30) -> Dict[str, Any]:\n        \"\"\"Generate a personalized study plan using AI\"\"\"\n        try:\n            user_analytics = user_model.get_learning_analytics()\n            \n            prompt = f\"\"\"Generate a {duration_days}-day personalized study plan for the {domain} domain.\n\nUser Profile:\n- Knowledge State: {json.dumps(user_analytics.get('knowledge_state', {}), indent=2)}\n- Overall Accuracy: {user_analytics.get('overall_accuracy', 0.5):.2f}\n- Learning Style Scores: {json.dumps(user_analytics.get('learning_style_scores', {}), indent=2)}\n- Mastered Topics: {user_analytics.get('mastered_topics', [])}\n- Struggling Topics: {user_analytics.get('struggling_topics', [])}\n- Cognitive Load Capacity: {user_analytics.get('cognitive_load_capacity', 0.7)}\n- Attention Span: {user_analytics.get('attention_span_minutes', 25)} minutes\n\nDomain Configuration:\n{json.dumps(config.get_domain_config(domain).__dict__ if config.get_domain_config(domain) else {}, indent=2)}\n\nCreate a comprehensive study plan with:\n1. Daily topics and time allocation\n2. Progressive difficulty adjustment\n3. Regular review and assessment points\n4. Accommodation for user's learning style and capacity\n5. Specific goals and milestones\n\nOutput as structured JSON with daily breakdowns.\n\"\"\"\n            \n            response = await self._call_ai_api(prompt)\n            \n            try:\n                plan_data = json.loads(response)\n            except:\n                # Fallback: extract JSON from response or create structured plan\n                plan_data = self._create_fallback_study_plan(user_model, domain, duration_days)\n                \n            return plan_data\n            \n        except Exception as e:\n            logger.error(f\"Error generating study plan: {str(e)}\")\n            return self._create_fallback_study_plan(user_model, domain, duration_days)\n            \n    def _create_fallback_study_plan(self, user_model: UserModel, domain: str, duration_days: int) -> Dict[str, Any]:\n        \"\"\"Create a fallback study plan\"\"\"\n        domain_config = config.get_domain_config(domain)\n        if not domain_config:\n            return {\"error\": \"Domain configuration not found\"}\n            \n        topics = domain_config.topics\n        struggling_topics = [topic for topic, prob in user_model.knowledge_state.items() if prob < 0.3]\n        \n        # Simple round-robin allocation\n        daily_plans = []\n        topics_to_focus = struggling_topics if struggling_topics else topics[:5]\n        \n        for day in range(1, duration_days + 1):\n            topic_index = (day - 1) % len(topics_to_focus)\n            current_topic = topics_to_focus[topic_index]\n            \n            daily_plan = {\n                \"day\": day,\n                \"topic\": current_topic,\n                \"activities\": [\n                    {\"type\": \"study\", \"duration\": 20, \"content\": f\"Review {current_topic} concepts\"},\n                    {\"type\": \"practice\", \"duration\": 25, \"content\": f\"Solve {current_topic} problems\"},\n                    {\"type\": \"review\", \"duration\": 10, \"content\": \"Review mistakes and key points\"}\n                ],\n                \"goals\": [f\"Improve understanding of {current_topic}\"],\n                \"assessment\": {\"type\": \"quiz\", \"questions\": 5}\n            }\n            daily_plans.append(daily_plan)\n            \n        return {\n            \"domain\": domain,\n            \"duration_days\": duration_days,\n            \"daily_plans\": daily_plans,\n            \"overall_goals\": [f\"Master key concepts in {domain}\", \"Improve problem-solving skills\"],\n            \"generated_at\": datetime.now().isoformat()\n        }\n\n# Helper functions for content management\nclass ContentManager:\n    \"\"\"Manages generated content storage and retrieval\"\"\"\n    \n    def __init__(self):\n        self.questions_cache = {}\n        self.content_cache = {}\n        \n    def cache_question(self, question: Question):\n        \"\"\"Cache generated question\"\"\"\n        cache_key = f\"{question.domain}_{question.topic}_{question.difficulty}_{question.question_type}\"\n        if cache_key not in self.questions_cache:\n            self.questions_cache[cache_key] = []\n        self.questions_cache[cache_key].append(question)\n        \n        # Keep only last 10 questions per cache key\n        self.questions_cache[cache_key] = self.questions_cache[cache_key][-10:]\n        \n    def get_cached_question(self, domain: str, topic: str, difficulty: str, question_type: str) -> Optional[Question]:\n        \"\"\"Get a random cached question matching criteria\"\"\"\n        cache_key = f\"{domain}_{topic}_{difficulty}_{question_type}\"\n        questions = self.questions_cache.get(cache_key, [])\n        return random.choice(questions) if questions else None\n        \n    def cache_content(self, content: LearningContent):\n        \"\"\"Cache generated content\"\"\"\n        cache_key = f\"{content.domain}_{content.topic}_{content.difficulty}_{content.content_type}\"\n        self.content_cache[cache_key] = content\n        \n    def get_cached_content(self, domain: str, topic: str, difficulty: str, content_type: str) -> Optional[LearningContent]:\n        \"\"\"Get cached content matching criteria\"\"\"\n        cache_key = f\"{domain}_{topic}_{difficulty}_{content_type}\"\n        return self.content_cache.get(cache_key)\n        \n    def export_cache(self) -> Dict[str, Any]:\n        \"\"\"Export cached content for storage\"\"\"\n        return {\n            \"questions\": {k: [asdict(q) for q in v] for k, v in self.questions_cache.items()},\n            \"content\": {k: asdict(v) for k, v in self.content_cache.items()},\n            \"exported_at\": datetime.now().isoformat()\n        }\n        \n    def import_cache(self, cache_data: Dict[str, Any]):\n        \"\"\"Import cached content from storage\"\"\"\n        if \"questions\" in cache_data:\n            for key, questions_data in cache_data[\"questions\"].items():\n                self.questions_cache[key] = [Question(**q) for q in questions_data]\n                \n        if \"content\" in cache_data:\n            for key, content_data in cache_data[\"content\"].items():\n                self.content_cache[key] = LearningContent(**content_data)\n\n# Global content manager instance\ncontent_manager = ContentManager()\n"
    },
    {
      "type": "writeFile",
      "path": "src/adaptive_learning/learning_interface.py",
      "ifExists": "overwrite",
      "description": "Main learning interface with domain selection and adaptive questioning",
      "content": "# Main Learning Interface for Adaptive Learning System\nimport asyncio\nimport json\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom datetime import datetime, timedelta\nimport logging\nfrom dataclasses import asdict\n\nfrom .config import config\nfrom .user_modeling import UserModel, UserInteraction\nfrom .reinforcement_learning import LearningEnvironment, AdaptivePathwayAgent, LearningState\nfrom .content_generation import AIContentGenerator, Question, LearningContent, content_manager\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass AdaptiveLearningInterface:\n    \"\"\"Main interface for the adaptive learning system\"\"\"\n    \n    def __init__(self, user_id: str):\n        self.user_id = user_id\n        self.user_model = UserModel(user_id)\n        self.current_domain = None\n        self.learning_environment = None\n        self.rl_agent = None\n        self.content_generator = None\n        self.session_active = False\n        self.current_session = {\n            \"start_time\": None,\n            \"questions_answered\": 0,\n            \"correct_answers\": 0,\n            \"total_time_spent\": 0.0,\n            \"topics_covered\": set(),\n            \"session_id\": None\n        }\n        \n    async def initialize(self):\n        \"\"\"Initialize the learning interface\"\"\"\n        try:\n            self.content_generator = AIContentGenerator()\n            await self.content_generator.__aenter__()\n            logger.info(f\"Initialized adaptive learning interface for user {self.user_id}\")\n        except Exception as e:\n            logger.error(f\"Error initializing interface: {str(e)}\")\n            raise\n            \n    async def cleanup(self):\n        \"\"\"Cleanup resources\"\"\"\n        if self.content_generator:\n            await self.content_generator.__aexit__(None, None, None)\n            \n    def get_available_domains(self) -> List[Dict[str, Any]]:\n        \"\"\"Get list of available learning domains\"\"\"\n        domains = []\n        for domain_id, domain_config in config.learning_domains.items():\n            domains.append({\n                \"id\": domain_id,\n                \"name\": domain_config.display_name,\n                \"description\": domain_config.description,\n                \"topics\": domain_config.topics,\n                \"user_progress\": self._get_domain_progress(domain_id)\n            })\n        return domains\n        \n    def _get_domain_progress(self, domain_id: str) -> Dict[str, Any]:\n        \"\"\"Calculate user's progress in a domain\"\"\"\n        domain_config = config.get_domain_config(domain_id)\n        if not domain_config:\n            return {\"overall_progress\": 0.0, \"topics_mastered\": 0, \"total_topics\": 0}\n            \n        topics = domain_config.topics\n        mastered_count = 0\n        total_progress = 0.0\n        \n        for topic in topics:\n            knowledge_prob = self.user_model.knowledge_state.get(topic, 0.0)\n            total_progress += knowledge_prob\n            if knowledge_prob > 0.85:  # Mastery threshold\n                mastered_count += 1\n                \n        return {\n            \"overall_progress\": total_progress / len(topics) if topics else 0.0,\n            \"topics_mastered\": mastered_count,\n            \"total_topics\": len(topics),\n            \"domain_interactions\": len([i for i in self.user_model.interactions_history \n                                       if i.domain == domain_id])\n        }\n        \n    async def select_domain(self, domain_id: str) -> Dict[str, Any]:\n        \"\"\"Select learning domain and initialize environment\"\"\"\n        domain_config = config.get_domain_config(domain_id)\n        if not domain_config:\n            return {\"success\": False, \"error\": \"Invalid domain selected\"}\n            \n        self.current_domain = domain_id\n        \n        # Initialize RL environment and agent\n        self.learning_environment = LearningEnvironment(self.user_model, domain_id)\n        self.rl_agent = AdaptivePathwayAgent(self.learning_environment)\n        \n        # Get personalized domain introduction\n        domain_progress = self._get_domain_progress(domain_id)\n        recommendations = await self._get_domain_recommendations(domain_id)\n        \n        return {\n            \"success\": True,\n            \"domain\": {\n                \"id\": domain_id,\n                \"name\": domain_config.display_name,\n                \"description\": domain_config.description\n            },\n            \"progress\": domain_progress,\n            \"recommendations\": recommendations,\n            \"next_steps\": await self._get_suggested_next_steps(domain_id)\n        }\n        \n    async def _get_domain_recommendations(self, domain_id: str) -> Dict[str, Any]:\n        \"\"\"Get AI-powered recommendations for the domain\"\"\"\n        try:\n            user_analytics = self.user_model.get_learning_analytics()\n            \n            # Identify focus areas\n            struggling_topics = [topic for topic, prob in self.user_model.knowledge_state.items() \n                               if prob < 0.3]\n            mastered_topics = [topic for topic, prob in self.user_model.knowledge_state.items() \n                             if prob > 0.85]\n            \n            recommendations = {\n                \"focus_areas\": struggling_topics[:3] if struggling_topics else [],\n                \"mastered_areas\": mastered_topics,\n                \"suggested_difficulty\": self._suggest_overall_difficulty(),\n                \"study_time_recommendation\": self._recommend_study_time(),\n                \"learning_style_tips\": self._get_learning_style_tips()\n            }\n            \n            return recommendations\n            \n        except Exception as e:\n            logger.error(f\"Error getting domain recommendations: {str(e)}\")\n            return {\n                \"focus_areas\": [],\n                \"mastered_areas\": [],\n                \"suggested_difficulty\": \"intermediate\",\n                \"study_time_recommendation\": 45,\n                \"learning_style_tips\": [\"Practice regularly\", \"Review mistakes\"]\n            }\n            \n    def _suggest_overall_difficulty(self) -> str:\n        \"\"\"Suggest overall difficulty level for user\"\"\"\n        if not self.user_model.knowledge_state:\n            return \"intermediate\"\n            \n        avg_knowledge = sum(self.user_model.knowledge_state.values()) / len(self.user_model.knowledge_state)\n        \n        if avg_knowledge < 0.3:\n            return \"beginner\"\n        elif avg_knowledge < 0.6:\n            return \"intermediate\"\n        elif avg_knowledge < 0.85:\n            return \"advanced\"\n        else:\n            return \"expert\"\n            \n    def _recommend_study_time(self) -> int:\n        \"\"\"Recommend daily study time in minutes\"\"\"\n        attention_span = self.user_model.attention_span\n        cognitive_capacity = self.user_model.cognitive_load_capacity\n        \n        # Base recommendation on attention span and cognitive capacity\n        base_time = min(attention_span * 2, 90)  # Max 90 minutes\n        adjusted_time = base_time * cognitive_capacity\n        \n        return max(15, min(120, int(adjusted_time)))  # Between 15 and 120 minutes\n        \n    def _get_learning_style_tips(self) -> List[str]:\n        \"\"\"Get personalized learning style tips\"\"\"\n        style_scores = self.user_model.learning_style_scores\n        dominant_style = max(style_scores, key=style_scores.get)\n        \n        tips_map = {\n            \"visual\": [\n                \"Use diagrams and flowcharts to understand concepts\",\n                \"Create mind maps for complex topics\",\n                \"Watch video tutorials and visual demonstrations\"\n            ],\n            \"auditory\": [\n                \"Listen to explanations and discussions\",\n                \"Practice explaining concepts out loud\",\n                \"Use mnemonic devices and verbal repetition\"\n            ],\n            \"kinesthetic\": [\n                \"Practice hands-on exercises and implementations\",\n                \"Take breaks and move around while studying\",\n                \"Use physical manipulatives when possible\"\n            ],\n            \"reading\": [\n                \"Read comprehensive explanations and documentation\",\n                \"Take detailed notes while learning\",\n                \"Create written summaries of key concepts\"\n            ]\n        }\n        \n        return tips_map.get(dominant_style, [\n            \"Practice regularly with varied exercises\",\n            \"Review and reinforce learned concepts\",\n            \"Seek feedback on your progress\"\n        ])\n        \n    async def _get_suggested_next_steps(self, domain_id: str) -> List[Dict[str, Any]]:\n        \"\"\"Get suggested next learning steps\"\"\"\n        domain_config = config.get_domain_config(domain_id)\n        if not domain_config:\n            return []\n            \n        next_steps = []\n        \n        # Find topics to focus on\n        for topic in domain_config.topics[:5]:  # Top 5 topics\n            knowledge_prob = self.user_model.knowledge_state.get(topic, 0.0)\n            difficulty = self.user_model.recommend_difficulty(topic, domain_id)\n            \n            if knowledge_prob < 0.85:  # Not mastered yet\n                next_steps.append({\n                    \"type\": \"practice\",\n                    \"topic\": topic,\n                    \"difficulty\": difficulty,\n                    \"estimated_time\": 15 + (5 * (difficulty == \"advanced\")) + (10 * (difficulty == \"expert\")),\n                    \"description\": f\"Practice {topic.replace('_', ' ')} at {difficulty} level\",\n                    \"priority\": 1.0 - knowledge_prob  # Higher priority for lower knowledge\n                })\n                \n        # Sort by priority\n        next_steps.sort(key=lambda x: x[\"priority\"], reverse=True)\n        return next_steps[:3]\n        \n    async def start_learning_session(self) -> Dict[str, Any]:\n        \"\"\"Start a new learning session\"\"\"\n        if not self.current_domain:\n            return {\"success\": False, \"error\": \"No domain selected\"}\n            \n        session_id = f\"session_{self.user_id}_{int(datetime.now().timestamp())}\"\n        self.current_session = {\n            \"session_id\": session_id,\n            \"start_time\": datetime.now(),\n            \"questions_answered\": 0,\n            \"correct_answers\": 0,\n            \"total_time_spent\": 0.0,\n            \"topics_covered\": set(),\n            \"domain\": self.current_domain\n        }\n        \n        self.session_active = True\n        \n        # Get first question using RL agent\n        first_question = await self._get_next_question()\n        \n        return {\n            \"success\": True,\n            \"session_id\": session_id,\n            \"question\": asdict(first_question) if first_question else None,\n            \"session_info\": {\n                \"domain\": self.current_domain,\n                \"estimated_session_length\": self._recommend_study_time(),\n                \"focus_areas\": await self._get_current_focus_areas()\n            }\n        }\n        \n    async def _get_current_focus_areas(self) -> List[str]:\n        \"\"\"Get current focus areas for the session\"\"\"\n        domain_config = config.get_domain_config(self.current_domain)\n        if not domain_config:\n            return []\n            \n        # Get topics sorted by need for improvement\n        topic_priorities = []\n        for topic in domain_config.topics:\n            knowledge_prob = self.user_model.knowledge_state.get(topic, 0.5)\n            recent_performance = self.user_model._get_recent_topic_performance(topic)\n            priority = (1.0 - knowledge_prob) * 0.7 + (1.0 - recent_performance) * 0.3\n            topic_priorities.append((topic, priority))\n            \n        # Sort by priority and return top 3\n        topic_priorities.sort(key=lambda x: x[1], reverse=True)\n        return [topic for topic, _ in topic_priorities[:3]]\n        \n    async def _get_next_question(self) -> Optional[Question]:\n        \"\"\"Get the next question using RL-based recommendation\"\"\"\n        if not self.learning_environment or not self.rl_agent:\n            # Fallback to simple selection\n            return await self._get_fallback_question()\n            \n        try:\n            # Get current state\n            current_state = self._build_current_learning_state()\n            \n            # Get RL recommendation\n            recommended_action = self.rl_agent.get_recommended_action(current_state)\n            \n            # Generate question based on recommendation\n            if recommended_action.action_type == \"question\":\n                question = await self._generate_question_for_action(recommended_action)\n                return question\n            elif recommended_action.action_type == \"review\":\n                # Generate review question\n                return await self._generate_review_question()\n            elif recommended_action.action_type == \"break\":\n                # Suggest break (return None to indicate break)\n                return None\n            else:\n                # Fallback\n                return await self._get_fallback_question()\n                \n        except Exception as e:\n            logger.error(f\"Error getting next question: {str(e)}\")\n            return await self._get_fallback_question()\n            \n    def _build_current_learning_state(self) -> LearningState:\n        \"\"\"Build current learning state for RL agent\"\"\"\n        # Calculate current session metrics\n        session_length = 0.0\n        if self.current_session[\"start_time\"]:\n            session_length = (datetime.now() - self.current_session[\"start_time\"]).total_seconds() / 60.0\n            \n        # Get recent performance\n        recent_interactions = self.user_model.interactions_history[-10:]\n        consecutive_correct = 0\n        consecutive_incorrect = 0\n        \n        for interaction in reversed(recent_interactions):\n            if interaction.is_correct:\n                if consecutive_incorrect == 0:\n                    consecutive_correct += 1\n                else:\n                    break\n            else:\n                if consecutive_correct == 0:\n                    consecutive_incorrect += 1\n                else:\n                    break\n                    \n        # Estimate engagement and energy\n        engagement_score = min(1.0, self.current_session[\"correct_answers\"] / \n                             max(1, self.current_session[\"questions_answered\"])) if self.current_session[\"questions_answered\"] > 0 else 0.8\n        energy_level = max(0.1, 1.0 - (session_length / 120.0))  # Decays over 2 hours\n        \n        # Get average knowledge for current domain topics\n        domain_config = config.get_domain_config(self.current_domain)\n        avg_knowledge = 0.5\n        if domain_config:\n            topic_knowledge = [self.user_model.knowledge_state.get(topic, 0.5) \n                             for topic in domain_config.topics]\n            avg_knowledge = sum(topic_knowledge) / len(topic_knowledge) if topic_knowledge else 0.5\n            \n        return LearningState(\n            user_id=self.user_id,\n            current_topic=\"general\",  # Will be updated when question is selected\n            current_domain=self.current_domain,\n            knowledge_level=avg_knowledge,\n            difficulty_level=self._suggest_overall_difficulty(),\n            time_spent_today=self._get_today_study_time(),\n            consecutive_correct=consecutive_correct,\n            consecutive_incorrect=consecutive_incorrect,\n            engagement_score=engagement_score,\n            energy_level=energy_level,\n            session_length=session_length\n        )\n        \n    def _get_today_study_time(self) -> float:\n        \"\"\"Get total study time for today in minutes\"\"\"\n        today = datetime.now().date()\n        today_interactions = [\n            i for i in self.user_model.interactions_history\n            if datetime.fromtimestamp(i.timestamp).date() == today\n        ]\n        return sum(i.response_time for i in today_interactions) / 60.0\n        \n    async def _generate_question_for_action(self, action) -> Question:\n        \"\"\"Generate question based on RL action\"\"\"\n        user_context = {\n            \"knowledge_level\": self.user_model.knowledge_state.get(action.topic, 0.5),\n            \"preferred_learning_style\": max(self.user_model.learning_style_scores,\n                                          key=self.user_model.learning_style_scores.get),\n            \"recent_topics\": list(self.current_session[\"topics_covered\"])\n        }\n        \n        # Try to get from cache first\n        cached_question = content_manager.get_cached_question(\n            self.current_domain, action.topic, action.difficulty, action.content_type\n        )\n        \n        if cached_question:\n            return cached_question\n            \n        # Generate new question\n        question = await self.content_generator.generate_question(\n            domain=self.current_domain,\n            topic=action.topic,\n            difficulty=action.difficulty,\n            question_type=action.content_type,\n            user_context=user_context\n        )\n        \n        # Cache for future use\n        content_manager.cache_question(question)\n        return question\n        \n    async def _generate_review_question(self) -> Question:\n        \"\"\"Generate a review question from previously covered topics\"\"\"\n        covered_topics = list(self.current_session[\"topics_covered\"])\n        if not covered_topics:\n            return await self._get_fallback_question()\n            \n        review_topic = max(covered_topics, \n                         key=lambda t: self.user_model.knowledge_state.get(t, 0.5))\n        \n        return await self.content_generator.generate_question(\n            domain=self.current_domain,\n            topic=review_topic,\n            difficulty=\"intermediate\",\n            question_type=\"mcq\",\n            user_context={\"is_review\": True}\n        )\n        \n    async def _get_fallback_question(self) -> Question:\n        \"\"\"Get fallback question when RL system is unavailable\"\"\"\n        domain_config = config.get_domain_config(self.current_domain)\n        if not domain_config:\n            return None\n            \n        # Simple strategy: pick topic with lowest knowledge\n        topic_scores = {\n            topic: self.user_model.knowledge_state.get(topic, 0.5)\n            for topic in domain_config.topics\n        }\n        \n        weakest_topic = min(topic_scores, key=topic_scores.get)\n        difficulty = self.user_model.recommend_difficulty(weakest_topic, self.current_domain)\n        \n        return await self.content_generator.generate_question(\n            domain=self.current_domain,\n            topic=weakest_topic,\n            difficulty=difficulty,\n            question_type=\"mcq\"\n        )\n        \n    async def submit_answer(self, question_id: str, answer: str, \n                          response_time: float, confidence: float = 0.5) -> Dict[str, Any]:\n        \"\"\"Submit answer and get feedback\"\"\"\n        if not self.session_active:\n            return {\"success\": False, \"error\": \"No active learning session\"}\n            \n        try:\n            # Find question (in a real system, this would be stored)\n            question = None  # Would retrieve from storage/cache\n            \n            # Determine if answer is correct (simplified)\n            is_correct = self._evaluate_answer(answer, question) if question else True\n            \n            # Create interaction record\n            interaction = UserInteraction(\n                timestamp=datetime.now().timestamp(),\n                domain=self.current_domain,\n                topic=question.topic if question else \"general\",\n                question_id=question_id,\n                difficulty_level=question.difficulty if question else \"intermediate\",\n                response_time=response_time,\n                is_correct=is_correct,\n                confidence_score=confidence\n            )\n            \n            # Add to user model\n            self.user_model.add_interaction(interaction)\n            \n            # Update session stats\n            self.current_session[\"questions_answered\"] += 1\n            if is_correct:\n                self.current_session[\"correct_answers\"] += 1\n            self.current_session[\"total_time_spent\"] += response_time / 60.0\n            if question:\n                self.current_session[\"topics_covered\"].add(question.topic)\n                \n            # Get next question\n            next_question = await self._get_next_question()\n            \n            # Prepare feedback\n            feedback = {\n                \"is_correct\": is_correct,\n                \"explanation\": question.explanation if question else \"Good work!\",\n                \"hints\": question.hints if question and not is_correct else [],\n                \"knowledge_update\": self.user_model.knowledge_state.get(\n                    question.topic if question else \"general\", 0.5),\n                \"session_progress\": self._get_session_progress()\n            }\n            \n            return {\n                \"success\": True,\n                \"feedback\": feedback,\n                \"next_question\": asdict(next_question) if next_question else None,\n                \"continue_session\": next_question is not None\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error submitting answer: {str(e)}\")\n            return {\"success\": False, \"error\": \"Failed to process answer\"}\n            \n    def _evaluate_answer(self, user_answer: str, question: Question) -> bool:\n        \"\"\"Evaluate user's answer against correct answer\"\"\"\n        if not question:\n            return True  # Default to correct for testing\n            \n        if question.question_type == \"mcq\":\n            return user_answer.upper().strip() == question.correct_answer.upper().strip()\n        else:\n            # For subjective questions, would need more sophisticated evaluation\n            # Could use AI for evaluation or keyword matching\n            return len(user_answer.strip()) > 10  # Simplified check\n            \n    def _get_session_progress(self) -> Dict[str, Any]:\n        \"\"\"Get current session progress\"\"\"\n        session_duration = 0.0\n        if self.current_session[\"start_time\"]:\n            session_duration = (datetime.now() - self.current_session[\"start_time\"]).total_seconds() / 60.0\n            \n        accuracy = 0.0\n        if self.current_session[\"questions_answered\"] > 0:\n            accuracy = self.current_session[\"correct_answers\"] / self.current_session[\"questions_answered\"]\n            \n        return {\n            \"questions_answered\": self.current_session[\"questions_answered\"],\n            \"accuracy\": accuracy,\n            \"session_duration_minutes\": session_duration,\n            \"topics_covered\": len(self.current_session[\"topics_covered\"]),\n            \"estimated_remaining_time\": max(0, self._recommend_study_time() - session_duration)\n        }\n        \n    def end_session(self) -> Dict[str, Any]:\n        \"\"\"End the current learning session\"\"\"\n        if not self.session_active:\n            return {\"success\": False, \"error\": \"No active session to end\"}\n            \n        session_summary = {\n            \"session_id\": self.current_session[\"session_id\"],\n            \"duration_minutes\": (datetime.now() - self.current_session[\"start_time\"]).total_seconds() / 60.0,\n            \"questions_answered\": self.current_session[\"questions_answered\"],\n            \"accuracy\": self.current_session[\"correct_answers\"] / max(1, self.current_session[\"questions_answered\"]),\n            \"topics_covered\": list(self.current_session[\"topics_covered\"]),\n            \"domain\": self.current_domain,\n            \"ended_at\": datetime.now().isoformat()\n        }\n        \n        self.session_active = False\n        \n        # Generate session insights\n        insights = self._generate_session_insights(session_summary)\n        \n        return {\n            \"success\": True,\n            \"session_summary\": session_summary,\n            \"insights\": insights,\n            \"achievements\": self._check_session_achievements(session_summary)\n        }\n        \n    def _generate_session_insights(self, session_summary: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate insights from the learning session\"\"\"\n        insights = []\n        \n        accuracy = session_summary[\"accuracy\"]\n        if accuracy > 0.8:\n            insights.append(\"Excellent performance! You're mastering the material.\")\n        elif accuracy > 0.6:\n            insights.append(\"Good progress! Keep practicing to improve further.\")\n        else:\n            insights.append(\"Focus on understanding concepts before attempting more problems.\")\n            \n        duration = session_summary[\"duration_minutes\"]\n        if duration > 60:\n            insights.append(\"Long study session! Remember to take breaks to maintain focus.\")\n        elif duration < 10:\n            insights.append(\"Short session today. Try to study for longer periods when possible.\")\n            \n        topics_count = len(session_summary[\"topics_covered\"])\n        if topics_count > 3:\n            insights.append(\"You covered many topics today. Focus on fewer topics for deeper understanding.\")\n            \n        return insights\n        \n    def _check_session_achievements(self, session_summary: Dict[str, Any]) -> List[Dict[str, str]]:\n        \"\"\"Check for achievements earned in this session\"\"\"\n        achievements = []\n        \n        # Perfect score achievement\n        if session_summary[\"accuracy\"] == 1.0 and session_summary[\"questions_answered\"] >= 5:\n            achievements.append({\n                \"id\": \"perfect_session\",\n                \"name\": \"Perfect Session\",\n                \"description\": \"Got all questions right in a session!\",\n                \"icon\": \"\"\n            })\n            \n        # Marathon session achievement\n        if session_summary[\"duration_minutes\"] > 90:\n            achievements.append({\n                \"id\": \"marathon_session\",\n                \"name\": \"Marathon Learner\", \n                \"description\": \"Studied for over 90 minutes straight!\",\n                \"icon\": \"\"\n            })\n            \n        return achievements\n        \n    def get_user_analytics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive user analytics\"\"\"\n        return self.user_model.get_learning_analytics()\n        \n    def export_user_data(self) -> Dict[str, Any]:\n        \"\"\"Export all user data\"\"\"\n        return {\n            \"user_model\": self.user_model.export_user_data(),\n            \"interface_data\": {\n                \"current_domain\": self.current_domain,\n                \"session_active\": self.session_active,\n                \"current_session\": {\n                    **self.current_session,\n                    \"topics_covered\": list(self.current_session[\"topics_covered\"]),\n                    \"start_time\": self.current_session[\"start_time\"].isoformat() if self.current_session[\"start_time\"] else None\n                }\n            },\n            \"content_cache\": content_manager.export_cache()\n        }\n        \n    def clear_user_data(self):\n        \"\"\"Clear all user data\"\"\"\n        self.user_model.clear_user_data()\n        self.current_domain = None\n        self.session_active = False\n        self.current_session = {\n            \"start_time\": None,\n            \"questions_answered\": 0,\n            \"correct_answers\": 0,\n            \"total_time_spent\": 0.0,\n            \"topics_covered\": set(),\n            \"session_id\": None\n        }\n        logger.info(f\"Cleared all data for user {self.user_id}\")\n"
    },
    {
      "type": "writeFile",
      "path": "src/adaptive_learning/api_integration.py",
      "ifExists": "overwrite",
      "description": "API integration layer for external AI models and services",
      "content": "# API Integration for Adaptive Learning System\nimport asyncio\nimport aiohttp\nimport json\nfrom typing import Dict, List, Optional, Any, Union\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport logging\nimport time\nfrom urllib.parse import urljoin\n\nfrom .config import config\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass APIResponse:\n    \"\"\"Standard API response format\"\"\"\n    success: bool\n    data: Any = None\n    error: str = \"\"\n    response_time: float = 0.0\n    token_usage: Dict[str, int] = None\n    \nclass APIRateLimiter:\n    \"\"\"Simple rate limiter for API calls\"\"\"\n    \n    def __init__(self, max_calls_per_minute: int = 60):\n        self.max_calls = max_calls_per_minute\n        self.calls = []\n        \n    def can_make_call(self) -> bool:\n        \"\"\"Check if we can make a call without exceeding rate limit\"\"\"\n        now = time.time()\n        # Remove calls older than 1 minute\n        self.calls = [call_time for call_time in self.calls if now - call_time < 60]\n        return len(self.calls) < self.max_calls\n        \n    def record_call(self):\n        \"\"\"Record that a call was made\"\"\"\n        self.calls.append(time.time())\n        \n    async def wait_if_needed(self):\n        \"\"\"Wait if we need to respect rate limits\"\"\"\n        if not self.can_make_call():\n            # Calculate wait time\n            oldest_call = min(self.calls) if self.calls else time.time()\n            wait_time = 61 - (time.time() - oldest_call)\n            if wait_time > 0:\n                logger.info(f\"Rate limit reached, waiting {wait_time:.1f} seconds\")\n                await asyncio.sleep(wait_time)\n\nclass AIModelClient:\n    \"\"\"Client for interacting with various AI models via API\"\"\"\n    \n    def __init__(self, base_url: str = None, api_key: str = None, model_name: str = None):\n        self.base_url = base_url or config.api_config.base_url\n        self.api_key = api_key or config.api_config.api_key\n        self.model_name = model_name or config.api_config.model_name\n        self.session = None\n        self.rate_limiter = APIRateLimiter(max_calls_per_minute=50)\n        \n    async def __aenter__(self):\n        \"\"\"Async context manager entry\"\"\"\n        self.session = aiohttp.ClientSession()\n        return self\n        \n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Async context manager exit\"\"\"\n        if self.session:\n            await self.session.close()\n            \n    def configure(self, base_url: str, api_key: str, model_name: str = None):\n        \"\"\"Configure API connection parameters\"\"\"\n        self.base_url = base_url\n        self.api_key = api_key\n        if model_name:\n            self.model_name = model_name\n        logger.info(f\"Configured AI model client for {self.base_url}\")\n        \n    async def generate_text(self, prompt: str, system_prompt: str = None, \n                           max_tokens: int = None, temperature: float = None,\n                           **kwargs) -> APIResponse:\n        \"\"\"Generate text using the configured AI model\"\"\"\n        start_time = time.time()\n        \n        try:\n            # Wait for rate limiting if needed\n            await self.rate_limiter.wait_if_needed()\n            \n            if not self.base_url or not self.api_key:\n                return APIResponse(\n                    success=False,\n                    error=\"API not configured. Please provide base_url and api_key.\",\n                    response_time=time.time() - start_time\n                )\n                \n            # Build request payload\n            messages = []\n            if system_prompt:\n                messages.append({\"role\": \"system\", \"content\": system_prompt})\n            messages.append({\"role\": \"user\", \"content\": prompt})\n            \n            payload = {\n                \"model\": self.model_name,\n                \"messages\": messages,\n                \"max_tokens\": max_tokens or config.api_config.max_tokens,\n                \"temperature\": temperature or config.api_config.temperature,\n                **kwargs\n            }\n            \n            headers = {\n                \"Authorization\": f\"Bearer {self.api_key}\",\n                \"Content-Type\": \"application/json\"\n            }\n            \n            timeout = aiohttp.ClientTimeout(total=config.api_config.timeout)\n            \n            # Make API call\n            self.rate_limiter.record_call()\n            async with self.session.post(\n                urljoin(self.base_url, \"/chat/completions\"),\n                json=payload,\n                headers=headers,\n                timeout=timeout\n            ) as response:\n                \n                response_time = time.time() - start_time\n                \n                if response.status == 200:\n                    data = await response.json()\n                    content = data[\"choices\"][0][\"message\"][\"content\"]\n                    \n                    # Extract token usage if available\n                    token_usage = data.get(\"usage\", {})\n                    \n                    return APIResponse(\n                        success=True,\n                        data=content,\n                        response_time=response_time,\n                        token_usage=token_usage\n                    )\n                else:\n                    error_text = await response.text()\n                    logger.error(f\"API call failed: {response.status} - {error_text}\")\n                    return APIResponse(\n                        success=False,\n                        error=f\"API call failed: {response.status} - {error_text}\",\n                        response_time=response_time\n                    )\n                    \n        except asyncio.TimeoutError:\n            return APIResponse(\n                success=False,\n                error=\"Request timed out\",\n                response_time=time.time() - start_time\n            )\n        except Exception as e:\n            logger.error(f\"Error in generate_text: {str(e)}\")\n            return APIResponse(\n                success=False,\n                error=str(e),\n                response_time=time.time() - start_time\n            )\n            \n    async def generate_structured_response(self, prompt: str, response_schema: Dict[str, Any],\n                                         system_prompt: str = None) -> APIResponse:\n        \"\"\"Generate structured response (JSON) from AI model\"\"\"\n        # Add JSON format instruction to prompt\n        json_instruction = f\"\"\"\n        \nPlease respond with valid JSON that matches this schema:\n        {json.dumps(response_schema, indent=2)}\n        \nEnsure your response is properly formatted JSON that can be parsed.\n        \"\"\"\n        \n        enhanced_prompt = prompt + json_instruction\n        \n        response = await self.generate_text(\n            prompt=enhanced_prompt,\n            system_prompt=system_prompt,\n            temperature=0.3  # Lower temperature for more consistent JSON\n        )\n        \n        if response.success:\n            try:\n                # Try to extract and parse JSON from response\n                json_str = response.data\n                \n                # Find JSON boundaries\n                start_idx = json_str.find('{')\n                if start_idx == -1:\n                    start_idx = json_str.find('[')\n                    \n                if start_idx != -1:\n                    # Find matching end bracket\n                    bracket_count = 0\n                    start_char = json_str[start_idx]\n                    end_char = '}' if start_char == '{' else ']'\n                    \n                    for i, char in enumerate(json_str[start_idx:], start_idx):\n                        if char in '{[':\n                            bracket_count += 1\n                        elif char in '}]':\n                            bracket_count -= 1\n                            if bracket_count == 0:\n                                end_idx = i + 1\n                                json_str = json_str[start_idx:end_idx]\n                                break\n                                \n                parsed_data = json.loads(json_str)\n                return APIResponse(\n                    success=True,\n                    data=parsed_data,\n                    response_time=response.response_time,\n                    token_usage=response.token_usage\n                )\n                \n            except json.JSONDecodeError as e:\n                logger.error(f\"Failed to parse JSON response: {str(e)}\")\n                return APIResponse(\n                    success=False,\n                    error=f\"Failed to parse JSON: {str(e)}. Raw response: {response.data[:200]}...\",\n                    response_time=response.response_time\n                )\n        else:\n            return response\n            \n    async def batch_generate(self, prompts: List[str], system_prompt: str = None,\n                            max_concurrent: int = 5) -> List[APIResponse]:\n        \"\"\"Generate responses for multiple prompts concurrently\"\"\"\n        semaphore = asyncio.Semaphore(max_concurrent)\n        \n        async def generate_single(prompt: str) -> APIResponse:\n            async with semaphore:\n                return await self.generate_text(prompt, system_prompt)\n                \n        tasks = [generate_single(prompt) for prompt in prompts]\n        responses = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        # Convert exceptions to error responses\n        result = []\n        for i, response in enumerate(responses):\n            if isinstance(response, Exception):\n                result.append(APIResponse(\n                    success=False,\n                    error=f\"Exception in batch request {i}: {str(response)}\"\n                ))\n            else:\n                result.append(response)\n                \n        return result\n        \nclass SpecializedAIServices:\n    \"\"\"Specialized AI services for different learning domains\"\"\"\n    \n    def __init__(self, ai_client: AIModelClient):\n        self.ai_client = ai_client\n        \n    async def generate_explanation(self, topic: str, domain: str, difficulty: str,\n                                 user_context: Dict[str, Any] = None) -> APIResponse:\n        \"\"\"Generate educational explanation for a topic\"\"\"\n        system_prompt = \"\"\"\nYou are an expert educator specializing in creating clear, comprehensive explanations.\nAdapt your explanations to the user's level and learning style.\nUse examples, analogies, and step-by-step breakdowns when helpful.\n\"\"\"\n        \n        user_info = \"\"\n        if user_context:\n            knowledge_level = user_context.get('knowledge_level', 0.5)\n            learning_style = user_context.get('preferred_learning_style', 'mixed')\n            user_info = f\"\"\"\nUser Context:\n- Knowledge Level: {knowledge_level:.2f} (0.0 = beginner, 1.0 = expert)\n- Learning Style: {learning_style}\n- Domain: {domain}\n\"\"\"\n            \n        prompt = f\"\"\"\nCreate a comprehensive explanation of \"{topic}\" at {difficulty} level.\n\n{user_info}\n\nYour explanation should:\n1. Start with a clear definition\n2. Provide relevant examples\n3. Explain key concepts and relationships\n4. Include practical applications\n5. Suggest next steps for learning\n\nMake it engaging and appropriate for the user's level.\n\"\"\"\n        \n        return await self.ai_client.generate_text(prompt, system_prompt)\n        \n    async def evaluate_answer(self, question: str, student_answer: str, \n                            correct_answer: str, rubric: Dict[str, Any] = None) -> APIResponse:\n        \"\"\"Evaluate and provide feedback on student's answer\"\"\"\n        system_prompt = \"\"\"\nYou are an expert educator providing constructive feedback on student answers.\nBe encouraging while being accurate. Point out what's good and what needs improvement.\n\"\"\"\n        \n        rubric_text = \"\"\n        if rubric:\n            rubric_text = f\"\\nEvaluation Rubric: {json.dumps(rubric, indent=2)}\"\n            \n        prompt = f\"\"\"\nEvaluate the following student answer:\n\nQuestion: {question}\nStudent Answer: {student_answer}\nCorrect/Expected Answer: {correct_answer}{rubric_text}\n\nProvide a structured evaluation with:\n1. Overall correctness (0-100%)\n2. What the student got right\n3. What needs improvement\n4. Specific suggestions for improvement\n5. Encouragement and next steps\n\nBe constructive and educational in your feedback.\n\"\"\"\n        \n        schema = {\n            \"correctness_percentage\": \"number (0-100)\",\n            \"strengths\": \"list of strings\",\n            \"areas_for_improvement\": \"list of strings\",\n            \"suggestions\": \"list of strings\",\n            \"encouragement\": \"string\",\n            \"next_steps\": \"list of strings\"\n        }\n        \n        return await self.ai_client.generate_structured_response(prompt, schema, system_prompt)\n        \n    async def generate_study_plan(self, domain: str, user_profile: Dict[str, Any],\n                                duration_days: int, learning_goals: List[str]) -> APIResponse:\n        \"\"\"Generate personalized study plan\"\"\"\n        system_prompt = \"\"\"\nYou are an expert learning advisor creating personalized study plans.\nConsider the user's current knowledge, goals, and constraints.\nCreate practical, achievable plans with clear milestones.\n\"\"\"\n        \n        prompt = f\"\"\"\nCreate a {duration_days}-day study plan for {domain}.\n\nUser Profile:\n{json.dumps(user_profile, indent=2)}\n\nLearning Goals:\n{json.dumps(learning_goals, indent=2)}\n\nCreate a comprehensive plan including:\n1. Daily breakdown with topics and time allocation\n2. Weekly milestones and assessments\n3. Resource recommendations\n4. Adaptive elements based on progress\n5. Motivation and engagement strategies\n\nMake it practical and achievable.\n\"\"\"\n        \n        schema = {\n            \"total_duration_days\": \"number\",\n            \"weekly_themes\": \"list of objects with week_number, theme, and goals\",\n            \"daily_plans\": \"list of objects with day, topics, activities, duration, and resources\",\n            \"milestones\": \"list of objects with day, milestone_name, and assessment_criteria\",\n            \"adaptive_strategies\": \"list of strings\",\n            \"resource_recommendations\": \"object with categories and specific resources\"\n        }\n        \n        return await self.ai_client.generate_structured_response(prompt, schema, system_prompt)\n        \n    async def generate_hint(self, question: str, difficulty: str, \n                          user_struggle_points: List[str] = None) -> APIResponse:\n        \"\"\"Generate progressive hints for a question\"\"\"\n        system_prompt = \"\"\"\nYou are an expert tutor providing helpful hints.\nGive hints that guide thinking without giving away the answer.\nStart with general guidance and get more specific if needed.\n\"\"\"\n        \n        struggle_info = \"\"\n        if user_struggle_points:\n            struggle_info = f\"\\nUser seems to struggle with: {', '.join(user_struggle_points)}\"\n            \n        prompt = f\"\"\"\nCreate progressive hints for this question at {difficulty} level:\n\nQuestion: {question}{struggle_info}\n\nGenerate 3 hints:\n1. A general hint that points toward the right approach\n2. A more specific hint that narrows down the solution method\n3. A detailed hint that guides step-by-step thinking\n\nEach hint should build on the previous one without giving away the full answer.\n\"\"\"\n        \n        schema = {\n            \"hint_1_general\": \"string\",\n            \"hint_2_specific\": \"string\", \n            \"hint_3_detailed\": \"string\",\n            \"learning_objectives\": \"list of strings\"\n        }\n        \n        return await self.ai_client.generate_structured_response(prompt, schema, system_prompt)\n        \nclass APIIntegrationManager:\n    \"\"\"Main manager for all API integrations\"\"\"\n    \n    def __init__(self):\n        self.ai_client = AIModelClient()\n        self.specialized_services = None\n        self.connection_status = \"disconnected\"\n        \n    async def initialize(self, base_url: str, api_key: str, model_name: str = None):\n        \"\"\"Initialize API connections\"\"\"\n        try:\n            self.ai_client.configure(base_url, api_key, model_name)\n            await self.ai_client.__aenter__()\n            self.specialized_services = SpecializedAIServices(self.ai_client)\n            \n            # Test connection\n            test_response = await self.ai_client.generate_text(\n                \"Hello, this is a connection test.\", \n                max_tokens=10\n            )\n            \n            if test_response.success:\n                self.connection_status = \"connected\"\n                logger.info(\"API integration initialized successfully\")\n                return {\"success\": True, \"message\": \"API connected successfully\"}\n            else:\n                self.connection_status = \"error\"\n                logger.error(f\"API connection test failed: {test_response.error}\")\n                return {\"success\": False, \"error\": f\"Connection test failed: {test_response.error}\"}\n                \n        except Exception as e:\n            self.connection_status = \"error\"\n            logger.error(f\"Failed to